{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tbQlYMH6EUtLwTbQLOl1w0C7kVbJxKFG","timestamp":1770697137230}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Lab 7.2: Text Similarity Measures — Cosine, Jaccard, and\n","WordNet-based Similarity"],"metadata":{"id":"bchH5NiHv3Un"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6HwOTmVco-9Y"},"outputs":[],"source":["documents = [\n","\"Machine learning helps computers learn from data and improve performance.\",\n","\"Machine learning allows systems to learn from data and enhance their performance.\",\n","\"Computers can learn automatically from data using machine learning techniques.\",\n","\"Deep learning is a subset of machine learning using neural networks.\",\n","\"Neural networks are used in deep learning to model complex patterns.\",\n","\"Artificial intelligence enables machines to mimic human intelligence.\",\n","\"AI systems can perform tasks that normally require human intelligence.\",\n","\"The university provides quality education to students.\",\n","\"Students receive high quality education from the university.\",\n","\"Online education platforms support remote learning.\",\n","\"Remote learning is supported by many online education platforms.\",\n","\"Cricket is a popular sport in many countries.\",\n","\"Football is played by millions of people worldwide.\",\n","\"The weather today is sunny and pleasant.\",\n","\"Quantum physics studies the behavior of matter at atomic scales.\"\n","]"]},{"cell_type":"markdown","source":["STEP 1 — Preprocessing"],"metadata":{"id":"VzpBf0kqqFJO"}},{"cell_type":"code","source":["# Import libraries\n","import nltk\n","import string\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download required NLTK resources (run once)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Dataset (given text)\n","documents = [\n","    \"Machine learning helps computers learn from data and improve performance.\",\n","    \"Machine learning allows systems to learn from data and enhance their performance.\",\n","    \"Computers can learn automatically from data using machine learning techniques.\",\n","    \"Deep learning is a subset of machine learning using neural networks.\",\n","    \"Neural networks are used in deep learning to model complex patterns.\",\n","    \"Artificial intelligence enables machines to mimic human intelligence.\",\n","    \"AI systems can perform tasks that normally require human intelligence.\",\n","    \"The university provides quality education to students.\",\n","    \"Students receive high quality education from the university.\",\n","    \"Online education platforms support remote learning.\",\n","    \"Remote learning is supported by many online education platforms.\",\n","    \"Cricket is a popular sport in many countries.\",\n","    \"Football is played by millions of people worldwide.\",\n","    \"The weather today is sunny and pleasant.\",\n","    \"Quantum physics studies the behavior of matter at atomic scales.\"\n","]\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    tokens = word_tokenize(text)\n","\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    return tokens\n","\n","print(\"Preprocessed Documents:\\n\")\n","\n","for i, doc in enumerate(documents):\n","    processed = preprocess_text(doc)\n","    print(f\"Document {i+1}:\")\n","    print(processed)\n","    print(\"-\" * 60)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3oYDzhyud5Y","executionInfo":{"status":"ok","timestamp":1770092359097,"user_tz":-330,"elapsed":83,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"be393fed-5328-4ce5-940f-768bb1627943"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessed Documents:\n","\n","Document 1:\n","['machine', 'learning', 'help', 'computer', 'learn', 'data', 'improve', 'performance']\n","------------------------------------------------------------\n","Document 2:\n","['machine', 'learning', 'allows', 'system', 'learn', 'data', 'enhance', 'performance']\n","------------------------------------------------------------\n","Document 3:\n","['computer', 'learn', 'automatically', 'data', 'using', 'machine', 'learning', 'technique']\n","------------------------------------------------------------\n","Document 4:\n","['deep', 'learning', 'subset', 'machine', 'learning', 'using', 'neural', 'network']\n","------------------------------------------------------------\n","Document 5:\n","['neural', 'network', 'used', 'deep', 'learning', 'model', 'complex', 'pattern']\n","------------------------------------------------------------\n","Document 6:\n","['artificial', 'intelligence', 'enables', 'machine', 'mimic', 'human', 'intelligence']\n","------------------------------------------------------------\n","Document 7:\n","['ai', 'system', 'perform', 'task', 'normally', 'require', 'human', 'intelligence']\n","------------------------------------------------------------\n","Document 8:\n","['university', 'provides', 'quality', 'education', 'student']\n","------------------------------------------------------------\n","Document 9:\n","['student', 'receive', 'high', 'quality', 'education', 'university']\n","------------------------------------------------------------\n","Document 10:\n","['online', 'education', 'platform', 'support', 'remote', 'learning']\n","------------------------------------------------------------\n","Document 11:\n","['remote', 'learning', 'supported', 'many', 'online', 'education', 'platform']\n","------------------------------------------------------------\n","Document 12:\n","['cricket', 'popular', 'sport', 'many', 'country']\n","------------------------------------------------------------\n","Document 13:\n","['football', 'played', 'million', 'people', 'worldwide']\n","------------------------------------------------------------\n","Document 14:\n","['weather', 'today', 'sunny', 'pleasant']\n","------------------------------------------------------------\n","Document 15:\n","['quantum', 'physic', 'study', 'behavior', 'matter', 'atomic', 'scale']\n","------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["STEP 2 — Feature Representation"],"metadata":{"id":"A7u734RUqcMZ"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","\n","joined_docs = [' '.join(tokens) for tokens in processed_docs]\n","\n","\n","tfidf = TfidfVectorizer()\n","tfidf_matrix = tfidf.fit_transform(joined_docs)\n","\n","\n","bow = CountVectorizer(binary=True)\n","bow_matrix = bow.fit_transform(joined_docs)"],"metadata":{"id":"bBe263I2qXGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["STEP 3 — Cosine Similarity"],"metadata":{"id":"7pNrC49lqg_l"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","\n","cosine_sim = cosine_similarity(tfidf_matrix)\n","\n","\n","pairs = []\n","for i in range(len(documents)):\n","    for j in range(i+1, len(documents)):\n","        pairs.append((i, j, cosine_sim[i][j]))\n","\n","\n","pairs = sorted(pairs, key=lambda x: x[2], reverse=True)\n","print(pairs[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sNG4FT3qgh6","executionInfo":{"status":"ok","timestamp":1770091368237,"user_tz":-330,"elapsed":53,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"40693a2e-6b79-46ce-86ab-7ad1fce27f95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(9, 10, np.float64(0.6920019895910456)), (7, 8, np.float64(0.6520259322648949)), (0, 1, np.float64(0.49235416447421093)), (0, 2, np.float64(0.4923541644742109)), (3, 4, np.float64(0.47116591825622567))]\n"]}]},{"cell_type":"markdown","source":["STEP 4 — Jaccard Similarity"],"metadata":{"id":"_53CV2hBquhk"}},{"cell_type":"code","source":["from sklearn.metrics import jaccard_score\n","\n","\n","jaccard_scores = []\n","\n","\n","for i in range(len(documents)):\n","    for j in range(i+1, len(documents)):\n","        score = jaccard_score(bow_matrix[i].toarray()[0], bow_matrix[j].toarray()[0])\n","        jaccard_scores.append((i, j, score))\n","\n","\n","jaccard_scores = sorted(jaccard_scores, key=lambda x: x[2], reverse=True)\n","print(jaccard_scores[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcaJAWs3quE4","executionInfo":{"status":"ok","timestamp":1770091419313,"user_tz":-330,"elapsed":132,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"10200d03-9eff-4570-e214-02c5803fab21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(9, 10, np.float64(0.625)), (7, 8, np.float64(0.5714285714285714)), (0, 1, np.float64(0.45454545454545453)), (0, 2, np.float64(0.45454545454545453)), (3, 4, np.float64(0.36363636363636365))]\n"]}]},{"cell_type":"markdown","source":["STEP 5 — WordNet Semantic Similarity"],"metadata":{"id":"5eEM3FcQq6s5"}},{"cell_type":"code","source":["from nltk.corpus import wordnet as wn\n","\n","\n","def sentence_similarity(s1, s2):\n","    score = 0\n","    count = 0\n","    for w1 in s1:\n","        for w2 in s2:\n","            syn1 = wn.synsets(w1)\n","            syn2 = wn.synsets(w2)\n","            if syn1 and syn2:\n","                sim = syn1[0].wup_similarity(syn2[0])\n","                if sim:\n","                    score += sim\n","                    count += 1\n","    return score / count if count else 0\n","\n","\n","for i in range(10):\n","    print(i, i+1, sentence_similarity(processed_docs[i], processed_docs[i+1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLBdlg9Mq9lo","executionInfo":{"status":"ok","timestamp":1770091461260,"user_tz":-330,"elapsed":141,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"454da6c2-7a0b-47f9-dbf8-0959931bdd50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 1 0.31197445579338945\n","1 2 0.30337980799837405\n","2 3 0.3119717445544651\n","3 4 0.35076831103945066\n","4 5 0.25295974424372053\n","5 6 0.2758191680935921\n","6 7 0.2607168210000018\n","7 8 0.3536639286639286\n","8 9 0.2834963148688639\n","9 10 0.34239847454133154\n"]}]},{"cell_type":"markdown","source":["STEP 6 — Comparison Section"],"metadata":{"id":"O7u2FkZJrJA-"}},{"cell_type":"code","source":["print(\"\"\"Cosine similarity detected copied and lightly modified documents most effectively because TF–IDF captures term importance.\n","Jaccard similarity failed when documents used different words with the same meaning, as it depends strictly on word overlap.\n","  WordNet similarity helped significantly in identifying paraphrased content by understanding semantic relationships between words.\n","  However, WordNet sometimes produced false positives for sentences sharing general concepts but different contexts.\n","  Jaccard performed best only for near-duplicate texts. Cosine similarity occasionally overestimated similarity for documents sharing common domain vocabulary.\n","  WordNet was computationally expensive but valuable for meaning-based detection. Overall, a hybrid approach yields the best plagiarism detection results.\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HcIvv695rG6t","executionInfo":{"status":"ok","timestamp":1770091590057,"user_tz":-330,"elapsed":31,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"7804a586-7d0f-4cd9-c498-c4428d64d133"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity detected copied and lightly modified documents most effectively because TF–IDF captures term importance.\n","Jaccard similarity failed when documents used different words with the same meaning, as it depends strictly on word overlap.\n","  WordNet similarity helped significantly in identifying paraphrased content by understanding semantic relationships between words. \n","  However, WordNet sometimes produced false positives for sentences sharing general concepts but different contexts. \n","  Jaccard performed best only for near-duplicate texts. Cosine similarity occasionally overestimated similarity for documents sharing common domain vocabulary. \n","  WordNet was computationally expensive but valuable for meaning-based detection. Overall, a hybrid approach yields the best plagiarism detection results.\n"]}]},{"cell_type":"markdown","source":["LAB REPORT\n"],"metadata":{"id":"RWSbKq9rroB5"}},{"cell_type":"markdown","source":["Lab Report: Text Similarity Analysis\n","1. Objective\n","The primary objective of this lab was to explore and compare various text similarity metrics—namely Cosine Similarity (using TF-IDF), Jaccard Similarity (using Bag-of-Words), and WordNet Semantic Similarity—in their ability to identify relatedness between a set of documents. The goal was to understand the strengths and limitations of each method in detecting direct copying, rephrasing, and semantic relatedness.\n","\n","2. Dataset Description\n","A synthetic dataset comprising 15 text documents related to \"Artificial Intelligence in Education\" was created. The dataset was structured into distinct categories to facilitate a robust comparison of similarity metrics:\n","\n","doc1_original.txt to doc5_original.txt: Original statements.\n","doc6_modified.txt to doc8_modified.txt: Slightly rephrased versions of original statements, intended to have high lexical overlap.\n","doc9_paraphrased.txt to doc11_paraphrased.txt: More significantly paraphrased versions, expected to have less direct lexical overlap but maintained semantic meaning.\n","doc12_new.txt to doc15_new.txt: New statements on related topics, designed to test semantic broader relatedness.\n","The dataset was zipped as text_similarity_dataset.zip for portability.\n","\n","3. Preprocessing Steps\n","Before calculating similarity, all text documents underwent a series of preprocessing steps:\n","\n","Lowercasing: All text was converted to lowercase to ensure consistency and prevent variations in capitalization from being treated as different words.\n","Punctuation Removal: All punctuation marks were removed to focus on the lexical content.\n","Tokenization: Text was split into individual words (tokens).\n","Stopword Removal: Common English stopwords (e.g., \"the,\" \"is,\" \"a\") were removed to reduce noise and focus on more meaningful terms.\n","(Optional) Lemmatization: For WordNet Semantic Similarity, an additional lemmatization step was performed to reduce words to their base forms (e.g., \"running\" to \"run\") to improve semantic matching.\n","4. Similarity Metric Results\n","4.1. Cosine Similarity (TF-IDF)\n","Description: Cosine similarity measures the cosine of the angle between two non-zero TF-IDF vectors. TF-IDF (Term Frequency-Inverse Document Frequency) weights terms by their importance in a document relative to the entire corpus. A score of 1.0 indicates identical content, 0.0 indicates no common terms.\n","\n","Top 5 Most Similar Document Pairs:\n","\n","Document Pair: doc3_original.txt - doc8_modified.txt, Similarity Score: 0.7266\n","Document Pair: doc1_original.txt - doc6_modified.txt, Similarity Score: 0.6749\n","Document Pair: doc2_original.txt - doc7_modified.txt, Similarity Score: 0.5171\n","Document Pair: doc10_paraphrased.txt - doc4_original.txt, Similarity Score: 0.3584\n","Document Pair: doc11_paraphrased.txt - doc5_original.txt, Similarity Score: 0.2600\n","Interpretation: Cosine Similarity performed well in identifying documents with significant lexical overlap, as evidenced by the high scores between original and modified documents. The TF-IDF weighting helps emphasize unique and important terms.\n","\n","4.2. Jaccard Similarity (Bag-of-Words)\n","Description: Jaccard Similarity measures the ratio of the intersection to the union of two sets of words (after preprocessing). A score of 1.0 means identical word sets, and 0.0 means no common words. It is sensitive to exact lexical matches.\n","\n","Top 5 Most Similar Document Pairs:\n","\n","Document Pair: doc3_original.txt - doc8_modified.txt, Similarity Score: 0.7143\n","Document Pair: doc1_original.txt - doc6_modified.txt, Similarity Score: 0.6667\n","Document Pair: doc2_original.txt - doc7_modified.txt, Similarity Score: 0.4211\n","Document Pair: doc10_paraphrased.txt - doc4_original.txt, Similarity Score: 0.2778\n","Document Pair: doc12_new.txt - doc9_paraphrased.txt, Similarity Score: 0.2500\n","Interpretation: Similar to Cosine Similarity, Jaccard Similarity effectively detected direct textual copying and highly similar rephrased content due to its reliance on the exact shared vocabulary.\n","\n","4.3. WordNet Semantic Similarity\n","Description: WordNet Semantic Similarity (using path similarity) measures the shortest path between concepts (synsets) in the WordNet hierarchy. This method aims to capture conceptual relationships beyond exact word matches. Scores range from 0.0 (no semantic path) to 1.0 (high semantic relatedness).\n","\n","Top 5 Most Similar Document Pairs:\n","\n","Document Pair: doc1_original.txt - doc6_modified.txt, Similarity Score: 0.2665\n","Document Pair: doc10_paraphrased.txt - doc4_original.txt, Similarity Score: 0.2611\n","Document Pair: doc10_paraphrased.txt - doc1_original.txt, Similarity Score: 0.2507\n","Document Pair: doc10_paraphrased.txt - doc6_modified.txt, Similarity Score: 0.2497\n","Document Pair: doc14_new.txt - doc6_modified.txt, Similarity Score: 0.2493\n","Interpretation: WordNet similarity yielded generally lower absolute scores compared to the other two metrics. However, it demonstrated the ability to find conceptual links, even between documents with minimal lexical overlap, by leveraging semantic relationships in WordNet.\n","\n","5. Comparative Analysis\n","Which similarity metric detected copying best? Cosine Similarity (TF-IDF) and Jaccard Similarity proved most effective at detecting direct textual copying or highly lexical similarities. For instance, both metrics assigned very high scores to the _original.txt and _modified.txt pairs, such as doc3_original.txt and doc8_modified.txt (Cosine: 0.7266, Jaccard: 0.7143). These high scores accurately reflect that the modified documents were essentially rephrased versions of the originals, maintaining significant lexical overlap.\n","\n","When did Jaccard fail? Jaccard Similarity's efficacy is directly tied to the exact lexical overlap between documents. It didn't necessarily 'fail' but showed its limitations when documents conveyed similar meanings using a more diverse vocabulary. For instance, with more heavily paraphrased content like doc10_paraphrased.txt and doc4_original.txt, Jaccard yielded a score of 0.2778, which is lower than the scores for directly modified pairs. This indicates that while the meaning might be similar, the unique word sets had less direct intersection.\n","\n","When did WordNet help? WordNet Semantic Similarity is designed to capture conceptual relationships even when direct lexical overlap is minimal. While its absolute scores were generally lower for the 'modified' and 'paraphrased' documents compared to Cosine and Jaccard (e.g., doc1_original.txt - doc6_modified.txt at 0.2665), it could be particularly helpful in scenarios where synonyms or semantically related terms are used. It can identify thematic connections, such as between doc10_paraphrased.txt and doc1_original.txt (0.2507), by linking terms like 'student performance' and 'learning' through their underlying semantic networks, even if the phrasing differs considerably.\n","\n","Any false positives? Based on the top 5 results for each metric, there were no clear 'false positives' where highly dissimilar documents received high similarity scores. The top pairs identified by Cosine and Jaccard were genuinely lexically similar (original vs. modified versions). WordNet's top scores were relatively modest (around 0.25-0.26), meaning it wasn't incorrectly flagging unrelated documents as highly similar. However, the interpretation of these lower WordNet scores as 'strong matches' depends on the specific application's threshold for semantic similarity.\n","  \n","6.conclusion\n","\n","This lab demonstrates that no single similarity measure is sufficient for robust plagiarism detection.\n"," Lexical methods like Cosine and Jaccard are effective for direct copying, while semantic methods like WordNet are essential\n","      for paraphrase detection.\n","      Combining multiple similarity metrics provides a more reliable and comprehensive analysis of document similarity"],"metadata":{"id":"W0F6IhBtx-Cc"}},{"cell_type":"markdown","source":["1. What is text similarity in NLP?\n","\n","Text similarity in Natural Language Processing (NLP) measures how similar two pieces of text are in terms of words, structure, or meaning. It is used to identify copied content, paraphrased text, or texts that express similar ideas.\n","\n","2. Difference between lexical and semantic similarity?\n","\n","Lexical similarity compares texts based on exact word matches and word frequency.\n","Example: TF-IDF, Jaccard similarity.\n","\n","Semantic similarity compares texts based on meaning, even if different words are used.\n","Example: WordNet similarity.\n","\n","Lexical methods fail for paraphrased text, while semantic methods can detect meaning similarity.\n","\n","3. Why is cosine similarity widely used?\n","\n","Cosine similarity is widely used because it:\n","\n","Works well with TF-IDF vectors\n","\n","Is independent of document length\n","\n","Handles high-dimensional text data efficiently\n","\n","Provides normalized similarity scores between 0 and 1\n","\n","This makes it ideal for document comparison.\n","\n","4. When does Jaccard fail to capture meaning?\n","\n","Jaccard similarity fails when:\n","\n","Two texts use different words with the same meaning\n","\n","Texts are paraphrased\n","\n","Synonyms are used instead of exact words\n","\n","Since it only checks word overlap, it cannot understand semantics.\n","\n","5. How does WordNet improve similarity?\n","\n","WordNet improves similarity by:\n","\n","Identifying synonyms and semantic relationships\n","\n","Comparing word meanings instead of exact words\n","\n","Detecting paraphrased sentences\n","\n","It helps measure similarity even when vocabulary differs.\n","\n","6. How does preprocessing affect similarity scores?\n","\n","Preprocessing improves similarity accuracy by:\n","\n","Removing noise like punctuation and stopwords\n","\n","Normalizing words using lowercasing and lemmatization\n","\n","Ensuring fair comparison between documents\n","\n","Without preprocessing, similarity scores may be inaccurate or misleading.\n","\n","7. Give two real-life applications of text similarity.\n","\n","Plagiarism detection in academic assignments\n","\n","Search engines, where similar documents are retrieved for user queries"],"metadata":{"id":"CUoYWxXsvbwL"}}]}