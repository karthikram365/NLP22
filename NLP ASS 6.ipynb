{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19bByt--Spvcsg3DAVU_bYn4TH4ooyh3j","timestamp":1770093044312}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyZCMhm4m6fB","executionInfo":{"status":"ok","timestamp":1770090403535,"user_tz":-330,"elapsed":45,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"b831a942-eaba-485c-b0d2-62563bd3d2d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Created /mnt/data/text_similarity_dataset/doc1_original.txt\n","Created /mnt/data/text_similarity_dataset/doc2_original.txt\n","Created /mnt/data/text_similarity_dataset/doc3_original.txt\n","Created /mnt/data/text_similarity_dataset/doc4_original.txt\n","Created /mnt/data/text_similarity_dataset/doc5_original.txt\n","Created /mnt/data/text_similarity_dataset/doc6_modified.txt\n","Created /mnt/data/text_similarity_dataset/doc7_modified.txt\n","Created /mnt/data/text_similarity_dataset/doc8_modified.txt\n","Created /mnt/data/text_similarity_dataset/doc9_paraphrased.txt\n","Created /mnt/data/text_similarity_dataset/doc10_paraphrased.txt\n","Created /mnt/data/text_similarity_dataset/doc11_paraphrased.txt\n","Created /mnt/data/text_similarity_dataset/doc12_new.txt\n","Created /mnt/data/text_similarity_dataset/doc13_new.txt\n","Created /mnt/data/text_similarity_dataset/doc14_new.txt\n","Created /mnt/data/text_similarity_dataset/doc15_new.txt\n","Created /mnt/data/text_similarity_dataset/README.md\n","Created /mnt/data/text_similarity_dataset.zip\n","Dataset created and zipped successfully!\n"]}],"source":["import os\n","import zipfile\n","\n","base_path = \"/mnt/data/text_similarity_dataset\"\n","os.makedirs(base_path, exist_ok=True)\n","\n","documents = {\n","    \"doc1_original.txt\": \"Artificial Intelligence is transforming education by enabling personalized learning experiences. AI systems can analyze student performance and adapt content to individual needs.\",\n","    \"doc2_original.txt\": \"AI-based tools help teachers by automating grading, tracking attendance, and identifying students who need additional support.\",\n","    \"doc3_original.txt\": \"Educational institutions are using artificial intelligence to enhance online learning platforms and improve student engagement.\",\n","    \"doc4_original.txt\": \"Machine learning algorithms can predict student outcomes by analyzing historical academic data and learning patterns.\",\n","    \"doc5_original.txt\": \"Artificial Intelligence reduces administrative workload in schools, allowing educators to focus more on teaching.\",\n","    \"doc6_modified.txt\": \"Artificial Intelligence is changing education by providing personalized learning experiences. AI systems analyze student performance and adjust content to meet individual needs.\",\n","    \"doc7_modified.txt\": \"AI-powered tools assist teachers by automating grading tasks, monitoring attendance, and identifying students requiring extra support.\",\n","    \"doc8_modified.txt\": \"Educational institutions use artificial intelligence to improve online learning platforms and increase student engagement.\",\n","    \"doc9_paraphrased.txt\": \"AI technology allows learning systems to customize educational content based on each student’s strengths and weaknesses.\",\n","    \"doc10_paraphrased.txt\": \"By studying past academic records, machine learning models can estimate future student performance and learning outcomes.\",\n","    \"doc11_paraphrased.txt\": \"Schools benefit from artificial intelligence by minimizing paperwork and giving teachers more time to focus on instruction.\",\n","    \"doc12_new.txt\": \"The integration of AI into educational systems is creating dynamic learning environments that cater to diverse student populations.\",\n","    \"doc13_new.txt\": \"Smart algorithms are now capable of recommending personalized learning paths, enhancing student motivation and retention.\",\n","    \"doc14_new.txt\": \"Virtual reality and augmented reality, powered by AI, offer immersive educational experiences that were previously unimaginable.\",\n","    \"doc15_new.txt\": \"Ethical considerations in AI development for education emphasize fairness, transparency, and data privacy to protect student information.\"\n","}\n","\n","# Create the text documents\n","for filename, content in documents.items():\n","    file_path = os.path.join(base_path, filename)\n","    with open(file_path, \"w\") as f:\n","        f.write(content)\n","    print(f\"Created {file_path}\")\n","\n","# Create a README file\n","readme_content = \"\"\"\n","# Text Similarity Dataset\n","\n","This dataset contains 15 text documents related to Artificial Intelligence in education.\n","\n","### Document Categories:\n","- `doc1_original.txt` to `doc5_original.txt`: Original statements about AI in education.\n","- `doc6_modified.txt` to `doc8_modified.txt`: Slightly rephrased versions of original statements.\n","- `doc9_paraphrased.txt` to `doc11_paraphrased.txt`: More significantly paraphrased versions.\n","- `doc12_new.txt` to `doc15_new.txt`: New statements on related topics.\n","\n","These documents can be used for tasks such as:\n","- Text similarity analysis\n","- Semantic search\n","- Paraphrase detection\n","- Topic modeling\n","\n","Created for demonstration purposes.\n","\"\"\"\n","\n","readme_file_path = os.path.join(base_path, \"README.md\")\n","with open(readme_file_path, \"w\") as f:\n","    f.write(readme_content)\n","print(f\"Created {readme_file_path}\")\n","\n","# Zip the files for download\n","zip_filename = \"text_similarity_dataset.zip\"\n","zip_file_path = os.path.join(\"/mnt/data\", zip_filename) # Store zip outside the base_path for easy download\n","\n","with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n","    for root, _, files in os.walk(base_path):\n","        for file in files:\n","            file_path = os.path.join(root, file)\n","            arcname = os.path.relpath(file_path, os.path.dirname(base_path)) # Path within the zip file\n","            zipf.write(file_path, arcname)\n","print(f\"Created {zip_file_path}\")\n","print(\"Dataset created and zipped successfully!\")"]},{"cell_type":"markdown","source":["# **STEP 1 — Preprocess**"],"metadata":{"id":"4PPA__BTnSs3"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffe88be8","executionInfo":{"status":"ok","timestamp":1770090607020,"user_tz":-330,"elapsed":282,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"6e73912c-c07e-4c1c-fa5f-e83890d8e6eb"},"source":["import nltk\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","# from nltk.stem import WordNetLemmatizer\n","\n","# Download necessary NLTK data (if not already downloaded)\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    nltk.download('stopwords')\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","\n","# Explicitly download 'punkt_tab' as it was requested in the traceback\n","# and seems to be causing issues even after 'punkt' download.\n","print(\"Explicitly attempting to download 'punkt_tab' as per traceback suggestion.\")\n","nltk.download('punkt_tab')\n","\n","# try:\n","#     nltk.data.find('corpora/wordnet')\n","# except LookupError:\n","#     nltk.download('wordnet')\n","\n","print(\"NLTK data (stopwords, punkt, punkt_tab) checked and downloaded if necessary.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Explicitly attempting to download 'punkt_tab' as per traceback suggestion.\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["NLTK data (stopwords, punkt, punkt_tab) checked and downloaded if necessary.\n"]}]},{"cell_type":"markdown","metadata":{"id":"fea5cf39"},"source":["### Preprocessing Function\n","\n","This function will perform the following steps:\n","1.  **Lowercase**: Convert all text to lowercase.\n","2.  **Punctuation Removal**: Remove all punctuation marks.\n","3.  **Tokenization**: Split the text into individual words.\n","4.  **Stopword Removal**: Remove common English stopwords.\n","5.  **(Optional) Lemmatization**: Convert words to their base form (commented out by default, uncomment to enable)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f93d1cf6","executionInfo":{"status":"ok","timestamp":1770090607608,"user_tz":-330,"elapsed":35,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"785de135-6ba1-4ed9-cee5-5d8c9954a298"},"source":["def preprocess_text(text):\n","    # 1. Lowercase\n","    text = text.lower()\n","\n","    # 2. Punctuation Removal\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # 3. Tokenization\n","    tokens = word_tokenize(text)\n","\n","    # 4. Stopword Removal\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # 5. (Optional) Lemmatization\n","    # lemmatizer = WordNetLemmatizer()\n","    # tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    return ' '.join(tokens)\n","\n","print(\"Preprocessing function defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessing function defined.\n"]}]},{"cell_type":"markdown","metadata":{"id":"80bd56ac"},"source":["### Apply Preprocessing to Documents\n","\n","Now, let's apply the `preprocess_text` function to all the documents and store the preprocessed versions."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83e55136","executionInfo":{"status":"ok","timestamp":1770090608293,"user_tz":-330,"elapsed":25,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"e4bd2d58-af90-4b31-f610-4afb85bff024"},"source":["import os\n","\n","# Assuming base_path and documents dict are still available from the previous cell\n","# If not, you might need to re-run the cell where they were defined or load them.\n","# For this example, we'll use the 'documents' dictionary that holds the original content.\n","\n","preprocessed_documents = {}\n","raw_documents = {}\n","\n","# Read content from the files and preprocess\n","for filename in os.listdir(base_path):\n","    if filename.endswith('.txt'): # Only process text files\n","        file_path = os.path.join(base_path, filename)\n","        with open(file_path, 'r') as f:\n","            content = f.read()\n","            raw_documents[filename] = content\n","            preprocessed_documents[filename] = preprocess_text(content)\n","\n","print(f\"Successfully preprocessed {len(preprocessed_documents)} documents.\")\n","\n","# Display original and preprocessed content for a sample document\n","sample_filename = \"doc1_original.txt\"\n","if sample_filename in raw_documents:\n","    print(f\"\\n--- Original Content of {sample_filename} ---\")\n","    print(raw_documents[sample_filename])\n","    print(f\"\\n--- Preprocessed Content of {sample_filename} ---\")\n","    print(preprocessed_documents[sample_filename])\n","else:\n","    print(f\"Sample file {sample_filename} not found in the loaded documents.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully preprocessed 15 documents.\n","\n","--- Original Content of doc1_original.txt ---\n","Artificial Intelligence is transforming education by enabling personalized learning experiences. AI systems can analyze student performance and adapt content to individual needs.\n","\n","--- Preprocessed Content of doc1_original.txt ---\n","artificial intelligence transforming education enabling personalized learning experiences ai systems analyze student performance adapt content individual needs\n"]}]},{"cell_type":"markdown","source":["# **STEP 2 — Feature Representation**"],"metadata":{"id":"TcvkEgUyn1jr"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import pandas as pd\n","\n","# Ensure documents are in a consistent order\n","document_filenames = sorted(preprocessed_documents.keys())\n","preprocessed_texts = [preprocessed_documents[filename] for filename in document_filenames]\n","\n","### TF-IDF for Cosine Similarity\n","print(\"\\n--- Generating TF-IDF Features ---\")\n","# Initialize TF-IDF Vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the preprocessed documents\n","tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_texts)\n","\n","print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n","print(\"First 5 TF-IDF features for the first document:\")\n","# Convert to DataFrame for better viewing\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=document_filenames)\n","print(tfidf_df.iloc[0, :].head().to_string())\n","\n","### Bag-of-Words for Jaccard Similarity\n","print(\"\\n--- Generating Bag-of-Words Features ---\")\n","# Initialize Count Vectorizer (Bag-of-Words)\n","cv_vectorizer = CountVectorizer()\n","\n","# Fit and transform the preprocessed documents\n","cv_matrix = cv_vectorizer.fit_transform(preprocessed_texts)\n","\n","print(f\"Bag-of-Words Matrix Shape: {cv_matrix.shape}\")\n","print(\"First 5 Bag-of-Words features for the first document:\")\n","# Convert to DataFrame for better viewing\n","cv_df = pd.DataFrame(cv_matrix.toarray(), columns=cv_vectorizer.get_feature_names_out(), index=document_filenames)\n","print(cv_df.iloc[0, :].head().to_string())\n","\n","print(\"Feature representation complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LtWTb0CBn6xj","executionInfo":{"status":"ok","timestamp":1770090670143,"user_tz":-330,"elapsed":11,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"20cee450-5997-4ad3-b6e1-4cc196b0e01c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Generating TF-IDF Features ---\n","TF-IDF Matrix Shape: (15, 116)\n","First 5 TF-IDF features for the first document:\n","academic          0.274958\n","adapt             0.000000\n","additional        0.000000\n","adjust            0.000000\n","administrative    0.000000\n","\n","--- Generating Bag-of-Words Features ---\n","Bag-of-Words Matrix Shape: (15, 116)\n","First 5 Bag-of-Words features for the first document:\n","academic          1\n","adapt             0\n","additional        0\n","adjust            0\n","administrative    0\n","Feature representation complete.\n"]}]},{"cell_type":"markdown","source":["# **STEP 3 — Cosine Similarity**"],"metadata":{"id":"e5pkfrP3oFws"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"id":"5b0166c8","executionInfo":{"status":"ok","timestamp":1770090768705,"user_tz":-330,"elapsed":59,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"250a93aa-7780-4e37-daa9-650481438151"},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","import pandas as pd\n","\n","print(\"\\n--- Calculating Cosine Similarity ---\")\n","\n","# Calculate cosine similarity matrix\n","# tfidf_matrix is already available from previous step\n","cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","\n","# Convert to DataFrame for better readability\n","cosine_sim_df = pd.DataFrame(cosine_sim_matrix,\n","                             index=document_filenames,\n","                             columns=document_filenames)\n","\n","print(\"Cosine Similarity Matrix (first 5x5 entries):\")\n","display(cosine_sim_df.head())\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Calculating Cosine Similarity ---\n","Cosine Similarity Matrix (first 5x5 entries):\n"]},{"output_type":"display_data","data":{"text/plain":["                       doc10_paraphrased.txt  doc11_paraphrased.txt  \\\n","doc10_paraphrased.txt               1.000000                    0.0   \n","doc11_paraphrased.txt               0.000000                    1.0   \n","doc12_new.txt                       0.070376                    0.0   \n","doc13_new.txt                       0.069993                    0.0   \n","doc14_new.txt                       0.000000                    0.0   \n","\n","                       doc12_new.txt  doc13_new.txt  doc14_new.txt  \\\n","doc10_paraphrased.txt       0.070376       0.069993       0.000000   \n","doc11_paraphrased.txt       0.000000       0.000000       0.000000   \n","doc12_new.txt               1.000000       0.048908       0.073873   \n","doc13_new.txt               0.048908       1.000000       0.000000   \n","doc14_new.txt               0.073873       0.000000       1.000000   \n","\n","                       doc15_new.txt  doc1_original.txt  doc2_original.txt  \\\n","doc10_paraphrased.txt       0.019108           0.124234           0.000000   \n","doc11_paraphrased.txt       0.000000           0.074081           0.061331   \n","doc12_new.txt               0.056628           0.134916           0.000000   \n","doc13_new.txt               0.020364           0.108365           0.000000   \n","doc14_new.txt               0.030300           0.084629           0.000000   \n","\n","                       doc3_original.txt  doc4_original.txt  \\\n","doc10_paraphrased.txt           0.076691           0.358380   \n","doc11_paraphrased.txt           0.087654           0.000000   \n","doc12_new.txt                   0.105497           0.074579   \n","doc13_new.txt                   0.053297           0.159553   \n","doc14_new.txt                   0.043505           0.000000   \n","\n","                       doc5_original.txt  doc6_modified.txt  \\\n","doc10_paraphrased.txt               0.00           0.118579   \n","doc11_paraphrased.txt               0.26           0.070709   \n","doc12_new.txt                       0.00           0.128775   \n","doc13_new.txt                       0.00           0.103433   \n","doc14_new.txt                       0.00           0.080777   \n","\n","                       doc7_modified.txt  doc8_modified.txt  \\\n","doc10_paraphrased.txt           0.000000           0.076691   \n","doc11_paraphrased.txt           0.058692           0.087654   \n","doc12_new.txt                   0.000000           0.105497   \n","doc13_new.txt                   0.000000           0.053297   \n","doc14_new.txt                   0.000000           0.043505   \n","\n","                       doc9_paraphrased.txt  \n","doc10_paraphrased.txt              0.072053  \n","doc11_paraphrased.txt              0.000000  \n","doc12_new.txt                      0.198750  \n","doc13_new.txt                      0.050073  \n","doc14_new.txt                      0.075633  "],"text/html":["\n","  <div id=\"df-8e5d6ce0-538a-4c51-a7c2-95e0e2986f66\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doc10_paraphrased.txt</th>\n","      <th>doc11_paraphrased.txt</th>\n","      <th>doc12_new.txt</th>\n","      <th>doc13_new.txt</th>\n","      <th>doc14_new.txt</th>\n","      <th>doc15_new.txt</th>\n","      <th>doc1_original.txt</th>\n","      <th>doc2_original.txt</th>\n","      <th>doc3_original.txt</th>\n","      <th>doc4_original.txt</th>\n","      <th>doc5_original.txt</th>\n","      <th>doc6_modified.txt</th>\n","      <th>doc7_modified.txt</th>\n","      <th>doc8_modified.txt</th>\n","      <th>doc9_paraphrased.txt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc10_paraphrased.txt</th>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.070376</td>\n","      <td>0.069993</td>\n","      <td>0.000000</td>\n","      <td>0.019108</td>\n","      <td>0.124234</td>\n","      <td>0.000000</td>\n","      <td>0.076691</td>\n","      <td>0.358380</td>\n","      <td>0.00</td>\n","      <td>0.118579</td>\n","      <td>0.000000</td>\n","      <td>0.076691</td>\n","      <td>0.072053</td>\n","    </tr>\n","    <tr>\n","      <th>doc11_paraphrased.txt</th>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.074081</td>\n","      <td>0.061331</td>\n","      <td>0.087654</td>\n","      <td>0.000000</td>\n","      <td>0.26</td>\n","      <td>0.070709</td>\n","      <td>0.058692</td>\n","      <td>0.087654</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>doc12_new.txt</th>\n","      <td>0.070376</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.048908</td>\n","      <td>0.073873</td>\n","      <td>0.056628</td>\n","      <td>0.134916</td>\n","      <td>0.000000</td>\n","      <td>0.105497</td>\n","      <td>0.074579</td>\n","      <td>0.00</td>\n","      <td>0.128775</td>\n","      <td>0.000000</td>\n","      <td>0.105497</td>\n","      <td>0.198750</td>\n","    </tr>\n","    <tr>\n","      <th>doc13_new.txt</th>\n","      <td>0.069993</td>\n","      <td>0.0</td>\n","      <td>0.048908</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.020364</td>\n","      <td>0.108365</td>\n","      <td>0.000000</td>\n","      <td>0.053297</td>\n","      <td>0.159553</td>\n","      <td>0.00</td>\n","      <td>0.103433</td>\n","      <td>0.000000</td>\n","      <td>0.053297</td>\n","      <td>0.050073</td>\n","    </tr>\n","    <tr>\n","      <th>doc14_new.txt</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.073873</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.030300</td>\n","      <td>0.084629</td>\n","      <td>0.000000</td>\n","      <td>0.043505</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>0.080777</td>\n","      <td>0.000000</td>\n","      <td>0.043505</td>\n","      <td>0.075633</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e5d6ce0-538a-4c51-a7c2-95e0e2986f66')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8e5d6ce0-538a-4c51-a7c2-95e0e2986f66 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8e5d6ce0-538a-4c51-a7c2-95e0e2986f66');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(cosine_sim_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"doc10_paraphrased.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4329444286151739,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.06999279368910694,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc11_paraphrased.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.44721359549995804,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0000000000000002,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc12_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.42663715400058466,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000004,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          0.07387319298424136\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc13_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.43500089078065407,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc14_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.44011831412690977,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.07387319298424136\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc15_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02067133137917127,\n        \"min\": 0.0,\n        \"max\": 0.056627728099481245,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          0.030299685304373512\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc1_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025724248908834364,\n        \"min\": 0.07408054580053415,\n        \"max\": 0.13491637591499062,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.07408054580053415,\n          0.08462901838482037\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc2_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027427871056080834,\n        \"min\": 0.0,\n        \"max\": 0.06133058415949569,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.06133058415949569,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc3_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02520946402148399,\n        \"min\": 0.043504744477970925,\n        \"max\": 0.10549712024685873,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.08765379518203952,\n          0.043504744477970925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc4_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14936623264125654,\n        \"min\": 0.0,\n        \"max\": 0.3583795129164709,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.15955347606264952\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc5_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11627547192293496,\n        \"min\": 0.0,\n        \"max\": 0.2599998593355508,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2599998593355508,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc6_modified.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.024553348015371255,\n        \"min\": 0.07070859206250717,\n        \"max\": 0.1287753334432932,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.07070859206250717,\n          0.08077692561465356\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc7_modified.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.026247973740886835,\n        \"min\": 0.0,\n        \"max\": 0.05869225355625242,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.05869225355625242,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc8_modified.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02520946402148399,\n        \"min\": 0.043504744477970925,\n        \"max\": 0.10549712024685873,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.08765379518203952,\n          0.043504744477970925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc9_paraphrased.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07327521038577017,\n        \"min\": 0.0,\n        \"max\": 0.19875027144329632,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          0.07563299075760746\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"470062c7"},"source":["### Highlighting Top 5 Most Similar Document Pairs\n","\n","Since the cosine similarity matrix is symmetric and the diagonal elements (document's similarity with itself) are always 1, we need to extract only the unique pairs (e.g., `docA` vs `docB` but not `docB` vs `docA`, and not `docA` vs `docA`)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"374eccf1","executionInfo":{"status":"ok","timestamp":1770090769367,"user_tz":-330,"elapsed":11,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"4587728a-3f04-468f-a107-c2a9b6ce1fe4"},"source":["# Extract unique pairs and their similarity scores\n","similarity_pairs = []\n","num_docs = len(document_filenames)\n","\n","for i in range(num_docs):\n","    for j in range(i + 1, num_docs): # Start from i+1 to avoid duplicates and self-similarity\n","        doc1 = document_filenames[i]\n","        doc2 = document_filenames[j]\n","        score = cosine_sim_df.loc[doc1, doc2]\n","        similarity_pairs.append(((doc1, doc2), score))\n","\n","# Sort pairs by similarity score in descending order\n","similarity_pairs.sort(key=lambda x: x[1], reverse=True)\n","\n","print(\"\\n--- Top 5 Most Similar Document Pairs (Cosine Similarity) ---\")\n","for pair, score in similarity_pairs[:5]:\n","    print(f\"Document Pair: {pair[0]} - {pair[1]}, Similarity Score: {score:.4f}\")\n","\n","print(\"\\n--- Interpretation of Scores ---\")\n","print(\"Cosine similarity measures the cosine of the angle between two non-zero vectors. In text analysis, these vectors are usually TF-IDF (or word count) vectors. A score of:\")\n","print(\"- 1.0 indicates identical content (or very similar meaning and word usage).\")\n","print(\"- 0.0 indicates no common terms (completely dissimilar).\")\n","print(\"- Values between 0.0 and 1.0 indicate varying degrees of similarity.\")\n","print(\"The higher the score, the more similar the documents are in terms of their semantic content (based on shared words and their importance).\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Top 5 Most Similar Document Pairs (Cosine Similarity) ---\n","Document Pair: doc3_original.txt - doc8_modified.txt, Similarity Score: 0.7266\n","Document Pair: doc1_original.txt - doc6_modified.txt, Similarity Score: 0.6749\n","Document Pair: doc2_original.txt - doc7_modified.txt, Similarity Score: 0.5171\n","Document Pair: doc10_paraphrased.txt - doc4_original.txt, Similarity Score: 0.3584\n","Document Pair: doc11_paraphrased.txt - doc5_original.txt, Similarity Score: 0.2600\n","\n","--- Interpretation of Scores ---\n","Cosine similarity measures the cosine of the angle between two non-zero vectors. In text analysis, these vectors are usually TF-IDF (or word count) vectors. A score of:\n","- 1.0 indicates identical content (or very similar meaning and word usage).\n","- 0.0 indicates no common terms (completely dissimilar).\n","- Values between 0.0 and 1.0 indicate varying degrees of similarity.\n","The higher the score, the more similar the documents are in terms of their semantic content (based on shared words and their importance).\n"]}]},{"cell_type":"markdown","source":["# **STEP 4 — Jaccard Similarity**"],"metadata":{"id":"6rMWijpcoiLt"}},{"cell_type":"code","source":["import pandas as pd\n","\n","print(\"\\n--- Calculating Jaccard Similarity ---\")\n","\n","def jaccard_similarity(text1, text2):\n","    # Convert preprocessed texts to sets of words\n","    words1 = set(text1.split())\n","    words2 = set(text2.split())\n","\n","    if not words1 and not words2:\n","        return 1.0 # Both empty, considered 100% similar\n","    if not words1 or not words2:\n","        return 0.0 # One empty, considered 0% similar (no overlap)\n","\n","    intersection = len(words1.intersection(words2))\n","    union = len(words1.union(words2))\n","    return intersection / union\n","\n","# Calculate Jaccard similarity matrix\n","jaccard_sim_matrix = []\n","num_docs = len(document_filenames)\n","\n","for i in range(num_docs):\n","    row = []\n","    for j in range(num_docs):\n","        score = jaccard_similarity(preprocessed_documents[document_filenames[i]],\n","                                   preprocessed_documents[document_filenames[j]])\n","        row.append(score)\n","    jaccard_sim_matrix.append(row)\n","\n","jaccard_sim_df = pd.DataFrame(jaccard_sim_matrix,\n","                              index=document_filenames,\n","                              columns=document_filenames)\n","\n","print(\"Jaccard Similarity Matrix (first 5x5 entries):\")\n","display(jaccard_sim_df.head())\n","\n","# Extract unique pairs and their similarity scores\n","jaccard_similarity_pairs = []\n","for i in range(num_docs):\n","    for j in range(i + 1, num_docs): # Start from i+1 to avoid duplicates and self-similarity\n","        doc1 = document_filenames[i]\n","        doc2 = document_filenames[j]\n","        score = jaccard_sim_df.loc[doc1, doc2]\n","        jaccard_similarity_pairs.append(((doc1, doc2), score))\n","\n","# Sort pairs by similarity score in descending order\n","jaccard_similarity_pairs.sort(key=lambda x: x[1], reverse=True)\n","\n","print(\"\\n--- Top 5 Most Similar Document Pairs (Jaccard Similarity) ---\")\n","for pair, score in jaccard_similarity_pairs[:5]:\n","    print(f\"Document Pair: {pair[0]} - {pair[1]}, Similarity Score: {score:.4f}\")\n","\n","print(\"\\n--- Interpretation of Scores ---\")\n","print(\"Jaccard Similarity measures the ratio of the intersection to the union of two sets of words. A score of:\")\n","print(\"- 1.0 indicates that the two documents share all of their unique words (identical word sets).\")\n","print(\"- 0.0 indicates that the two documents share no unique words (completely distinct word sets).\")\n","print(\"- Values between 0.0 and 1.0 indicate the proportion of shared unique words.\")\n","print(\"The higher the score, the greater the overlap in the unique vocabulary of the two documents.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"mq_ttWJrolhs","executionInfo":{"status":"ok","timestamp":1770090838295,"user_tz":-330,"elapsed":15,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"13bb922f-f3f7-4519-8f61-eff5520aafcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Calculating Jaccard Similarity ---\n","Jaccard Similarity Matrix (first 5x5 entries):\n"]},{"output_type":"display_data","data":{"text/plain":["                       doc10_paraphrased.txt  doc11_paraphrased.txt  \\\n","doc10_paraphrased.txt               1.000000                    0.0   \n","doc11_paraphrased.txt               0.000000                    1.0   \n","doc12_new.txt                       0.090909                    0.0   \n","doc13_new.txt                       0.095238                    0.0   \n","doc14_new.txt                       0.000000                    0.0   \n","\n","                       doc12_new.txt  doc13_new.txt  doc14_new.txt  \\\n","doc10_paraphrased.txt       0.090909       0.095238       0.000000   \n","doc11_paraphrased.txt       0.000000       0.000000       0.000000   \n","doc12_new.txt               1.000000       0.095238       0.095238   \n","doc13_new.txt               0.095238       1.000000       0.000000   \n","doc14_new.txt               0.095238       0.000000       1.000000   \n","\n","                       doc15_new.txt  doc1_original.txt  doc2_original.txt  \\\n","doc10_paraphrased.txt       0.041667           0.115385           0.000000   \n","doc11_paraphrased.txt       0.000000           0.076923           0.043478   \n","doc12_new.txt               0.086957           0.160000           0.000000   \n","doc13_new.txt               0.043478           0.120000           0.000000   \n","doc14_new.txt               0.043478           0.076923           0.000000   \n","\n","                       doc3_original.txt  doc4_original.txt  \\\n","doc10_paraphrased.txt           0.090909           0.277778   \n","doc11_paraphrased.txt           0.095238           0.000000   \n","doc12_new.txt                   0.142857           0.095238   \n","doc13_new.txt                   0.095238           0.157895   \n","doc14_new.txt                   0.045455           0.000000   \n","\n","                       doc5_original.txt  doc6_modified.txt  \\\n","doc10_paraphrased.txt           0.000000           0.111111   \n","doc11_paraphrased.txt           0.235294           0.074074   \n","doc12_new.txt                   0.000000           0.153846   \n","doc13_new.txt                   0.000000           0.115385   \n","doc14_new.txt                   0.000000           0.074074   \n","\n","                       doc7_modified.txt  doc8_modified.txt  \\\n","doc10_paraphrased.txt           0.000000           0.090909   \n","doc11_paraphrased.txt           0.041667           0.095238   \n","doc12_new.txt                   0.000000           0.142857   \n","doc13_new.txt                   0.000000           0.095238   \n","doc14_new.txt                   0.000000           0.045455   \n","\n","                       doc9_paraphrased.txt  \n","doc10_paraphrased.txt              0.086957  \n","doc11_paraphrased.txt              0.000000  \n","doc12_new.txt                      0.250000  \n","doc13_new.txt                      0.090909  \n","doc14_new.txt                      0.090909  "],"text/html":["\n","  <div id=\"df-026a5247-939d-453e-b0c2-b34e7a4185c2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doc10_paraphrased.txt</th>\n","      <th>doc11_paraphrased.txt</th>\n","      <th>doc12_new.txt</th>\n","      <th>doc13_new.txt</th>\n","      <th>doc14_new.txt</th>\n","      <th>doc15_new.txt</th>\n","      <th>doc1_original.txt</th>\n","      <th>doc2_original.txt</th>\n","      <th>doc3_original.txt</th>\n","      <th>doc4_original.txt</th>\n","      <th>doc5_original.txt</th>\n","      <th>doc6_modified.txt</th>\n","      <th>doc7_modified.txt</th>\n","      <th>doc8_modified.txt</th>\n","      <th>doc9_paraphrased.txt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc10_paraphrased.txt</th>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.090909</td>\n","      <td>0.095238</td>\n","      <td>0.000000</td>\n","      <td>0.041667</td>\n","      <td>0.115385</td>\n","      <td>0.000000</td>\n","      <td>0.090909</td>\n","      <td>0.277778</td>\n","      <td>0.000000</td>\n","      <td>0.111111</td>\n","      <td>0.000000</td>\n","      <td>0.090909</td>\n","      <td>0.086957</td>\n","    </tr>\n","    <tr>\n","      <th>doc11_paraphrased.txt</th>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.076923</td>\n","      <td>0.043478</td>\n","      <td>0.095238</td>\n","      <td>0.000000</td>\n","      <td>0.235294</td>\n","      <td>0.074074</td>\n","      <td>0.041667</td>\n","      <td>0.095238</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>doc12_new.txt</th>\n","      <td>0.090909</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.095238</td>\n","      <td>0.095238</td>\n","      <td>0.086957</td>\n","      <td>0.160000</td>\n","      <td>0.000000</td>\n","      <td>0.142857</td>\n","      <td>0.095238</td>\n","      <td>0.000000</td>\n","      <td>0.153846</td>\n","      <td>0.000000</td>\n","      <td>0.142857</td>\n","      <td>0.250000</td>\n","    </tr>\n","    <tr>\n","      <th>doc13_new.txt</th>\n","      <td>0.095238</td>\n","      <td>0.0</td>\n","      <td>0.095238</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.043478</td>\n","      <td>0.120000</td>\n","      <td>0.000000</td>\n","      <td>0.095238</td>\n","      <td>0.157895</td>\n","      <td>0.000000</td>\n","      <td>0.115385</td>\n","      <td>0.000000</td>\n","      <td>0.095238</td>\n","      <td>0.090909</td>\n","    </tr>\n","    <tr>\n","      <th>doc14_new.txt</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.095238</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.043478</td>\n","      <td>0.076923</td>\n","      <td>0.000000</td>\n","      <td>0.045455</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.074074</td>\n","      <td>0.000000</td>\n","      <td>0.045455</td>\n","      <td>0.090909</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-026a5247-939d-453e-b0c2-b34e7a4185c2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-026a5247-939d-453e-b0c2-b34e7a4185c2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-026a5247-939d-453e-b0c2-b34e7a4185c2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(\\\"The higher the score, the greater the overlap in the unique vocabulary of the two documents\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"doc10_paraphrased.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4289363962621733,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.09523809523809523,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc11_paraphrased.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.44721359549995804,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc12_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.41773658102592326,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.09523809523809523\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc13_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.42857142857142855,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.09523809523809523,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc14_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.43850912126557484,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.09523809523809523\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc15_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.030754446163639138,\n        \"min\": 0.0,\n        \"max\": 0.08695652173913043,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.043478260869565216\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc1_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03470245459660722,\n        \"min\": 0.07692307692307693,\n        \"max\": 0.16,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.07692307692307693,\n          0.12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc2_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.019444069369563388,\n        \"min\": 0.0,\n        \"max\": 0.043478260869565216,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.043478260869565216,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc3_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.034482904019095456,\n        \"min\": 0.045454545454545456,\n        \"max\": 0.14285714285714285,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.09523809523809523,\n          0.045454545454545456\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc4_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11703452631833115,\n        \"min\": 0.0,\n        \"max\": 0.2777777777777778,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.15789473684210525\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc5_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10522672835293127,\n        \"min\": 0.0,\n        \"max\": 0.23529411764705882,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.23529411764705882,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc6_modified.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03332237382840672,\n        \"min\": 0.07407407407407407,\n        \"max\": 0.15384615384615385,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.07407407407407407,\n          0.11538461538461539\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc7_modified.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018633899812498248,\n        \"min\": 0.0,\n        \"max\": 0.041666666666666664,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.041666666666666664,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc8_modified.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.034482904019095456,\n        \"min\": 0.045454545454545456,\n        \"max\": 0.14285714285714285,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.09523809523809523,\n          0.045454545454545456\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc9_paraphrased.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09050541928885684,\n        \"min\": 0.0,\n        \"max\": 0.25,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.09090909090909091\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- Top 5 Most Similar Document Pairs (Jaccard Similarity) ---\n","Document Pair: doc3_original.txt - doc8_modified.txt, Similarity Score: 0.7143\n","Document Pair: doc1_original.txt - doc6_modified.txt, Similarity Score: 0.6667\n","Document Pair: doc2_original.txt - doc7_modified.txt, Similarity Score: 0.4211\n","Document Pair: doc10_paraphrased.txt - doc4_original.txt, Similarity Score: 0.2778\n","Document Pair: doc12_new.txt - doc9_paraphrased.txt, Similarity Score: 0.2500\n","\n","--- Interpretation of Scores ---\n","Jaccard Similarity measures the ratio of the intersection to the union of two sets of words. A score of:\n","- 1.0 indicates that the two documents share all of their unique words (identical word sets).\n","- 0.0 indicates that the two documents share no unique words (completely distinct word sets).\n","- Values between 0.0 and 1.0 indicate the proportion of shared unique words.\n","The higher the score, the greater the overlap in the unique vocabulary of the two documents.\n"]}]},{"cell_type":"markdown","source":["# **STEP 5 — WordNet Semantic Similarity**"],"metadata":{"id":"OaoH5KqlovWZ"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"48a113ea","executionInfo":{"status":"ok","timestamp":1770090960767,"user_tz":-330,"elapsed":5563,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"afd1ac75-a3e8-4bd9-fce0-4d7761fdbbf7"},"source":["import os\n","\n","# Assuming base_path and documents dict are still available from the previous cell\n","# If not, you might need to re-run the cell where they were defined or load them.\n","# For this example, we'll use the 'documents' dictionary that holds the original content.\n","\n","preprocessed_documents = {}\n","raw_documents = {}\n","\n","# Read content from the files and preprocess\n","for filename in os.listdir(base_path):\n","    if filename.endswith('.txt'): # Only process text files\n","        file_path = os.path.join(base_path, filename)\n","        with open(file_path, 'r') as f:\n","            content = f.read()\n","            raw_documents[filename] = content\n","            preprocessed_documents[filename] = preprocess_text(content)\n","\n","print(f\"Successfully preprocessed {len(preprocessed_documents)} documents with lemmatization.\")\n","\n","# Display original and preprocessed content for a sample document\n","sample_filename = \"doc1_original.txt\"\n","if sample_filename in raw_documents:\n","    print(f\"\\n--- Original Content of {sample_filename} ---\")\n","    print(raw_documents[sample_filename])\n","    print(f\"\\n--- Preprocessed Content of {sample_filename} (with lemmatization) ---\")\n","    print(preprocessed_documents[sample_filename])\n","else:\n","    print(f\"Sample file {sample_filename} not found in the loaded documents.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully preprocessed 15 documents with lemmatization.\n","\n","--- Original Content of doc1_original.txt ---\n","Artificial Intelligence is transforming education by enabling personalized learning experiences. AI systems can analyze student performance and adapt content to individual needs.\n","\n","--- Preprocessed Content of doc1_original.txt (with lemmatization) ---\n","artificial intelligence transforming education enabling personalized learning experience ai system analyze student performance adapt content individual need\n"]}]},{"cell_type":"markdown","metadata":{"id":"0bf77b55"},"source":["**Reasoning**:\n","Now that the documents are preprocessed with lemmatization, the next step is to implement a WordNet-based semantic similarity function to compare document pairs. This involves defining a function that leverages NLTK's WordNet capabilities to calculate similarity between words and then aggregates these scores for full documents.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ab8a8200","executionInfo":{"status":"ok","timestamp":1770090970127,"user_tz":-330,"elapsed":27,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"15962c9e-258a-432d-8c01-d00e29c7ad47"},"source":["from nltk.corpus import wordnet as wn\n","from nltk.tokenize import word_tokenize\n","\n","def get_wordnet_synsets(word):\n","    \"\"\"Returns a list of WordNet synsets for a given word.\"\"\"\n","    return wn.synsets(word)\n","\n","def document_wordnet_similarity(doc1_text, doc2_text):\n","    \"\"\"Calculates a basic WordNet semantic similarity between two documents.\n","    This function tokenizes documents, finds synsets for each word, and then\n","    calculates the average path similarity between all possible word pairs.\n","    \"\"\"\n","    tokens1 = word_tokenize(doc1_text)\n","    tokens2 = word_tokenize(doc2_text)\n","\n","    # Filter out empty tokens or tokens that don't have synsets\n","    valid_tokens1 = [t for t in tokens1 if get_wordnet_synsets(t)]\n","    valid_tokens2 = [t for t in tokens2 if get_wordnet_synsets(t)]\n","\n","    if not valid_tokens1 or not valid_tokens2:\n","        return 0.0 # No common words with synsets, or one document is empty\n","\n","    similarity_scores = []\n","\n","    # Compare every valid word from doc1 with every valid word from doc2\n","    for t1 in valid_tokens1:\n","        for t2 in valid_tokens2:\n","            syns1 = get_wordnet_synsets(t1)\n","            syns2 = get_wordnet_synsets(t2)\n","\n","            # Find the highest similarity between any pair of synsets for the two words\n","            max_sim = 0.0\n","            for s1 in syns1:\n","                for s2 in syns2:\n","                    sim = s1.path_similarity(s2) # Using path similarity\n","                    if sim is not None and sim > max_sim:\n","                        max_sim = sim\n","            similarity_scores.append(max_sim)\n","\n","    if not similarity_scores:\n","        return 0.0\n","\n","    # Return the average of all similarity scores\n","    return sum(similarity_scores) / len(similarity_scores)\n","\n","print(\"WordNet semantic similarity function defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WordNet semantic similarity function defined.\n"]}]},{"cell_type":"markdown","metadata":{"id":"090665c6"},"source":["**Reasoning**:\n","Now that the WordNet semantic similarity function is defined, I will apply it to all document pairs to calculate their semantic similarity, store these scores in a DataFrame, and then identify and display the top 5 most similar pairs, along with an interpretation of the scores, as required by the task.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"0beaab40","executionInfo":{"status":"ok","timestamp":1770091048778,"user_tz":-330,"elapsed":67195,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"e89f39ba-d09e-4f40-deb6-3c7e7e1513b4"},"source":["import pandas as pd\n","\n","print(\"\\n--- Calculating WordNet Semantic Similarity ---\")\n","\n","# Ensure documents are in a consistent order\n","document_filenames = sorted(preprocessed_documents.keys())\n","\n","# Calculate WordNet semantic similarity matrix\n","wordnet_sim_matrix = []\n","num_docs = len(document_filenames)\n","\n","# Keep track of similarity pairs for sorting later\n","wordnet_similarity_pairs = []\n","\n","for i in range(num_docs):\n","    row = []\n","    for j in range(num_docs):\n","        doc1_name = document_filenames[i]\n","        doc2_name = document_filenames[j]\n","\n","        # Retrieve preprocessed text for each document\n","        doc1_text = preprocessed_documents[doc1_name]\n","        doc2_text = preprocessed_documents[doc2_name]\n","\n","        # Calculate similarity\n","        score = document_wordnet_similarity(doc1_text, doc2_text)\n","        row.append(score)\n","\n","        # Store unique pairs (j > i to avoid duplicates and self-similarity)\n","        if j > i:\n","            wordnet_similarity_pairs.append(((doc1_name, doc2_name), score))\n","    wordnet_sim_matrix.append(row)\n","\n","# Convert to DataFrame for better readability\n","wordnet_sim_df = pd.DataFrame(wordnet_sim_matrix,\n","                              index=document_filenames,\n","                              columns=document_filenames)\n","\n","print(\"WordNet Semantic Similarity Matrix (first 5x5 entries):\")\n","display(wordnet_sim_df.head())\n","\n","# Sort pairs by similarity score in descending order\n","wordnet_similarity_pairs.sort(key=lambda x: x[1], reverse=True)\n","\n","print(\"\\n--- Top 5 Most Similar Document Pairs (WordNet Semantic Similarity) ---\")\n","for pair, score in wordnet_similarity_pairs[:5]:\n","    print(f\"Document Pair: {pair[0]} - {pair[1]}, Similarity Score: {score:.4f}\")\n","\n","print(\"\\n--- Interpretation of Scores ---\")\n","print(\"WordNet semantic similarity (using path similarity) measures the shortest path between concepts (synsets) in the WordNet hierarchy. A score of:\")\n","print(\"- 1.0 indicates very high semantic relatedness (often direct synonyms or very closely related concepts).\")\n","print(\"- 0.0 indicates no semantic path found, suggesting words are entirely unrelated within the WordNet hierarchy, or one of the documents is empty of words with synsets.\")\n","print(\"- Values between 0.0 and 1.0 indicate varying degrees of semantic closeness, where a higher score means greater semantic similarity based on shared conceptual meaning rather than just shared words.\")\n","print(\"This method can capture relationships between words that are not explicitly shared but are semantically related (e.g., 'car' and 'vehicle').\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Calculating WordNet Semantic Similarity ---\n","WordNet Semantic Similarity Matrix (first 5x5 entries):\n"]},{"output_type":"display_data","data":{"text/plain":["                       doc10_paraphrased.txt  doc11_paraphrased.txt  \\\n","doc10_paraphrased.txt               0.318306               0.204419   \n","doc11_paraphrased.txt               0.204419               0.241621   \n","doc12_new.txt                       0.220020               0.169241   \n","doc13_new.txt                       0.217637               0.172046   \n","doc14_new.txt                       0.236689               0.192426   \n","\n","                       doc12_new.txt  doc13_new.txt  doc14_new.txt  \\\n","doc10_paraphrased.txt       0.220020       0.217637       0.236689   \n","doc11_paraphrased.txt       0.169241       0.172046       0.192426   \n","doc12_new.txt               0.236103       0.179439       0.213024   \n","doc13_new.txt               0.179439       0.238983       0.193297   \n","doc14_new.txt               0.213024       0.193297       0.326512   \n","\n","                       doc15_new.txt  doc1_original.txt  doc2_original.txt  \\\n","doc10_paraphrased.txt       0.163271           0.250674           0.218996   \n","doc11_paraphrased.txt       0.151910           0.207706           0.190039   \n","doc12_new.txt               0.150536           0.210844           0.189897   \n","doc13_new.txt               0.141651           0.209182           0.197134   \n","doc14_new.txt               0.157681           0.242911           0.204085   \n","\n","                       doc3_original.txt  doc4_original.txt  \\\n","doc10_paraphrased.txt           0.225734           0.261123   \n","doc11_paraphrased.txt           0.188530           0.176937   \n","doc12_new.txt                   0.189922           0.194698   \n","doc13_new.txt                   0.193686           0.198026   \n","doc14_new.txt                   0.207896           0.203462   \n","\n","                       doc5_original.txt  doc6_modified.txt  \\\n","doc10_paraphrased.txt           0.220680           0.249736   \n","doc11_paraphrased.txt           0.214335           0.211391   \n","doc12_new.txt                   0.177892           0.215553   \n","doc13_new.txt                   0.183206           0.211165   \n","doc14_new.txt                   0.199047           0.249347   \n","\n","                       doc7_modified.txt  doc8_modified.txt  \\\n","doc10_paraphrased.txt           0.210364           0.231589   \n","doc11_paraphrased.txt           0.182006           0.193571   \n","doc12_new.txt                   0.179828           0.194774   \n","doc13_new.txt                   0.185167           0.194579   \n","doc14_new.txt                   0.196651           0.215014   \n","\n","                       doc9_paraphrased.txt  \n","doc10_paraphrased.txt              0.223530  \n","doc11_paraphrased.txt              0.176926  \n","doc12_new.txt                      0.207034  \n","doc13_new.txt                      0.183071  \n","doc14_new.txt                      0.220943  "],"text/html":["\n","  <div id=\"df-ca888a8d-73ff-4d15-a1a8-550f23c36e42\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doc10_paraphrased.txt</th>\n","      <th>doc11_paraphrased.txt</th>\n","      <th>doc12_new.txt</th>\n","      <th>doc13_new.txt</th>\n","      <th>doc14_new.txt</th>\n","      <th>doc15_new.txt</th>\n","      <th>doc1_original.txt</th>\n","      <th>doc2_original.txt</th>\n","      <th>doc3_original.txt</th>\n","      <th>doc4_original.txt</th>\n","      <th>doc5_original.txt</th>\n","      <th>doc6_modified.txt</th>\n","      <th>doc7_modified.txt</th>\n","      <th>doc8_modified.txt</th>\n","      <th>doc9_paraphrased.txt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc10_paraphrased.txt</th>\n","      <td>0.318306</td>\n","      <td>0.204419</td>\n","      <td>0.220020</td>\n","      <td>0.217637</td>\n","      <td>0.236689</td>\n","      <td>0.163271</td>\n","      <td>0.250674</td>\n","      <td>0.218996</td>\n","      <td>0.225734</td>\n","      <td>0.261123</td>\n","      <td>0.220680</td>\n","      <td>0.249736</td>\n","      <td>0.210364</td>\n","      <td>0.231589</td>\n","      <td>0.223530</td>\n","    </tr>\n","    <tr>\n","      <th>doc11_paraphrased.txt</th>\n","      <td>0.204419</td>\n","      <td>0.241621</td>\n","      <td>0.169241</td>\n","      <td>0.172046</td>\n","      <td>0.192426</td>\n","      <td>0.151910</td>\n","      <td>0.207706</td>\n","      <td>0.190039</td>\n","      <td>0.188530</td>\n","      <td>0.176937</td>\n","      <td>0.214335</td>\n","      <td>0.211391</td>\n","      <td>0.182006</td>\n","      <td>0.193571</td>\n","      <td>0.176926</td>\n","    </tr>\n","    <tr>\n","      <th>doc12_new.txt</th>\n","      <td>0.220020</td>\n","      <td>0.169241</td>\n","      <td>0.236103</td>\n","      <td>0.179439</td>\n","      <td>0.213024</td>\n","      <td>0.150536</td>\n","      <td>0.210844</td>\n","      <td>0.189897</td>\n","      <td>0.189922</td>\n","      <td>0.194698</td>\n","      <td>0.177892</td>\n","      <td>0.215553</td>\n","      <td>0.179828</td>\n","      <td>0.194774</td>\n","      <td>0.207034</td>\n","    </tr>\n","    <tr>\n","      <th>doc13_new.txt</th>\n","      <td>0.217637</td>\n","      <td>0.172046</td>\n","      <td>0.179439</td>\n","      <td>0.238983</td>\n","      <td>0.193297</td>\n","      <td>0.141651</td>\n","      <td>0.209182</td>\n","      <td>0.197134</td>\n","      <td>0.193686</td>\n","      <td>0.198026</td>\n","      <td>0.183206</td>\n","      <td>0.211165</td>\n","      <td>0.185167</td>\n","      <td>0.194579</td>\n","      <td>0.183071</td>\n","    </tr>\n","    <tr>\n","      <th>doc14_new.txt</th>\n","      <td>0.236689</td>\n","      <td>0.192426</td>\n","      <td>0.213024</td>\n","      <td>0.193297</td>\n","      <td>0.326512</td>\n","      <td>0.157681</td>\n","      <td>0.242911</td>\n","      <td>0.204085</td>\n","      <td>0.207896</td>\n","      <td>0.203462</td>\n","      <td>0.199047</td>\n","      <td>0.249347</td>\n","      <td>0.196651</td>\n","      <td>0.215014</td>\n","      <td>0.220943</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca888a8d-73ff-4d15-a1a8-550f23c36e42')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ca888a8d-73ff-4d15-a1a8-550f23c36e42 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ca888a8d-73ff-4d15-a1a8-550f23c36e42');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(\\\"This method can capture relationships between words that are not explicitly shared but are semantically related (e\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"doc10_paraphrased.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.045569670753354774,\n        \"min\": 0.20441914340515738,\n        \"max\": 0.31830605292143754,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.20441914340515738,\n          0.23668881623427077,\n          0.22002002627002626\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc11_paraphrased.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029388459360922912,\n        \"min\": 0.16924109140018231,\n        \"max\": 0.2416206383148532,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2416206383148532,\n          0.1924264211454294,\n          0.16924109140018231\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc12_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02819181888606081,\n        \"min\": 0.16924109140018231,\n        \"max\": 0.23610259339426007,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.16924109140018231,\n          0.21302445450172725,\n          0.23610259339426007\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc13_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02774653174518146,\n        \"min\": 0.17204579167389086,\n        \"max\": 0.23898328485105344,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.17204579167389086,\n          0.19329660238751148,\n          0.17943949989404534\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc14_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.055627808529110326,\n        \"min\": 0.1924264211454294,\n        \"max\": 0.3265118719664174,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.1924264211454294,\n          0.3265118719664174,\n          0.21302445450172725\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc15_new.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008116018488996204,\n        \"min\": 0.14165091144106712,\n        \"max\": 0.16327132182509835,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.1519099551534222,\n          0.15768143369953325,\n          0.15053552740204323\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc1_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02077794816849228,\n        \"min\": 0.20770647949792334,\n        \"max\": 0.2506738359679536,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.20770647949792334,\n          0.24291058021004547,\n          0.21084390827037885\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc2_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012114468519936835,\n        \"min\": 0.18989656177156178,\n        \"max\": 0.21899581899581902,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.19003908549363094,\n          0.20408522453977,\n          0.18989656177156178\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc3_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.015739399685690738,\n        \"min\": 0.1885296731887641,\n        \"max\": 0.22573352573352573,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.1885296731887641,\n          0.20789551357733177,\n          0.18992153679653678\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc4_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.031927023858723154,\n        \"min\": 0.17693655250473433,\n        \"max\": 0.2611227447765909,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.17693655250473433,\n          0.20346246766701312,\n          0.19469845355262022\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc5_original.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018709504045828204,\n        \"min\": 0.17789240851740853,\n        \"max\": 0.22067987567987568,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.21433543728998275,\n          0.19904696313787223,\n          0.17789240851740853\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc6_modified.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.020253369412562592,\n        \"min\": 0.2111650975287339,\n        \"max\": 0.24973606640273305,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.21139058752695114,\n          0.24934718760476335,\n          0.21555301694190585\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc7_modified.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012712556070562327,\n        \"min\": 0.17982811348195965,\n        \"max\": 0.2103639949793796,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.18200560123637047,\n          0.1966508239235512,\n          0.17982811348195965\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc8_modified.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016933315876965418,\n        \"min\": 0.19357134868498505,\n        \"max\": 0.23158901283901281,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.19357134868498505,\n          0.21501404740041105,\n          0.19477387966971302\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc9_paraphrased.txt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02141423155501942,\n        \"min\": 0.17692580988035533,\n        \"max\": 0.22352978218362834,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.17692580988035533,\n          0.2209426516244698,\n          0.20703420807587475\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- Top 5 Most Similar Document Pairs (WordNet Semantic Similarity) ---\n","Document Pair: doc1_original.txt - doc6_modified.txt, Similarity Score: 0.2665\n","Document Pair: doc10_paraphrased.txt - doc4_original.txt, Similarity Score: 0.2611\n","Document Pair: doc10_paraphrased.txt - doc1_original.txt, Similarity Score: 0.2507\n","Document Pair: doc10_paraphrased.txt - doc6_modified.txt, Similarity Score: 0.2497\n","Document Pair: doc14_new.txt - doc6_modified.txt, Similarity Score: 0.2493\n","\n","--- Interpretation of Scores ---\n","WordNet semantic similarity (using path similarity) measures the shortest path between concepts (synsets) in the WordNet hierarchy. A score of:\n","- 1.0 indicates very high semantic relatedness (often direct synonyms or very closely related concepts).\n","- 0.0 indicates no semantic path found, suggesting words are entirely unrelated within the WordNet hierarchy, or one of the documents is empty of words with synsets.\n","- Values between 0.0 and 1.0 indicate varying degrees of semantic closeness, where a higher score means greater semantic similarity based on shared conceptual meaning rather than just shared words.\n","This method can capture relationships between words that are not explicitly shared but are semantically related (e.g., 'car' and 'vehicle').\n"]}]},{"cell_type":"markdown","source":["# **Step 6 -Comparison Section**"],"metadata":{"id":"TMYAt3EOo4Wf"}},{"cell_type":"markdown","source":["### Comparative Analysis of Similarity Metrics\n","\n","--- Which similarity metric detected copying best? ---\n","Cosine Similarity, driven by TF-IDF, and Jaccard Similarity proved most effective at detecting direct textual copying or highly lexical similarities. For instance, both metrics assigned very high scores to the `_original.txt` and `_modified.txt` pairs, such as `doc3_original.txt` and `doc8_modified.txt` (Cosine: 0.7266, Jaccard: 0.7143). These high scores accurately reflect that the modified documents were essentially rephrased versions of the originals, maintaining significant lexical overlap.\n","\n","--- When did Jaccard fail? ---\n","Jaccard Similarity's efficacy is directly tied to the exact lexical overlap between documents. It didn't necessarily 'fail' but showed its limitations when documents conveyed similar meanings using a more diverse vocabulary. For instance, with more heavily paraphrased content like `doc10_paraphrased.txt` and `doc4_original.txt`, Jaccard yielded a score of 0.2778, which is lower than the scores for directly modified pairs. This indicates that while the meaning might be similar, the unique word sets had less direct intersection.\n","\n","--- When did WordNet help? ---\n","WordNet Semantic Similarity is designed to capture conceptual relationships even when direct lexical overlap is minimal. While its absolute scores were generally lower for the 'modified' and 'paraphrased' documents compared to Cosine and Jaccard (e.g., `doc1_original.txt` - `doc6_modified.txt` at 0.2665), it could be particularly helpful in scenarios where synonyms or semantically related terms are used. It can identify thematic connections, such as between `doc10_paraphrased.txt` and `doc1_original.txt` (0.2507), by linking terms like 'student performance' and 'learning' through their underlying semantic networks, even if the phrasing differs considerably.\n","\n","--- Any false positives? ---\n","Based on the top 5 results for each metric, there were no clear 'false positives' where highly dissimilar documents received high similarity scores. The top pairs identified by Cosine and Jaccard were genuinely lexically similar (original vs. modified versions). WordNet's top scores were relatively modest (around 0.25-0.26), meaning it wasn't incorrectly flagging unrelated documents as highly similar. However, the interpretation of these lower WordNet scores as 'strong matches' depends on the specific application's threshold for semantic similarity."],"metadata":{"id":"0cy_aB2prNH_"}},{"cell_type":"markdown","source":["## **STEP 7 — Lab Report**"],"metadata":{"id":"E7Y7127JqgzC"}},{"cell_type":"markdown","metadata":{"id":"e70ad6ed"},"source":["# Task\n","The task is to generate a comprehensive lab report summarizing the objective, dataset, preprocessing steps, results from each similarity metric, comparative analysis, and conclusion of the text similarity project.\n","\n","Here's the lab report:\n","\n","## Lab Report: Text Similarity Analysis\n","\n","### 1. Objective\n","\n","The primary objective of this lab was to explore and compare various text similarity metrics—namely Cosine Similarity (using TF-IDF), Jaccard Similarity (using Bag-of-Words), and WordNet Semantic Similarity—in their ability to identify relatedness between a set of documents. The goal was to understand the strengths and limitations of each method in detecting direct copying, rephrasing, and semantic relatedness.\n","\n","### 2. Dataset Description\n","\n","A synthetic dataset comprising 15 text documents related to \"Artificial Intelligence in Education\" was created. The dataset was structured into distinct categories to facilitate a robust comparison of similarity metrics:\n","*   **`doc1_original.txt` to `doc5_original.txt`**: Original statements.\n","*   **`doc6_modified.txt` to `doc8_modified.txt`**: Slightly rephrased versions of original statements, intended to have high lexical overlap.\n","*   **`doc9_paraphrased.txt` to `doc11_paraphrased.txt`**: More significantly paraphrased versions, expected to have less direct lexical overlap but maintained semantic meaning.\n","*   **`doc12_new.txt` to `doc15_new.txt`**: New statements on related topics, designed to test semantic broader relatedness.\n","\n","The dataset was zipped as `text_similarity_dataset.zip` for portability.\n","\n","### 3. Preprocessing Steps\n","\n","Before calculating similarity, all text documents underwent a series of preprocessing steps:\n","1.  **Lowercasing**: All text was converted to lowercase to ensure consistency and prevent variations in capitalization from being treated as different words.\n","2.  **Punctuation Removal**: All punctuation marks were removed to focus on the lexical content.\n","3.  **Tokenization**: Text was split into individual words (tokens).\n","4.  **Stopword Removal**: Common English stopwords (e.g., \"the,\" \"is,\" \"a\") were removed to reduce noise and focus on more meaningful terms.\n","5.  **(Optional) Lemmatization**: For WordNet Semantic Similarity, an additional lemmatization step was performed to reduce words to their base forms (e.g., \"running\" to \"run\") to improve semantic matching.\n","\n","### 4. Similarity Metric Results\n","\n","#### 4.1. Cosine Similarity (TF-IDF)\n","\n","**Description**: Cosine similarity measures the cosine of the angle between two non-zero TF-IDF vectors. TF-IDF (Term Frequency-Inverse Document Frequency) weights terms by their importance in a document relative to the entire corpus. A score of 1.0 indicates identical content, 0.0 indicates no common terms.\n","\n","**Top 5 Most Similar Document Pairs**:\n","*   Document Pair: `doc3_original.txt` - `doc8_modified.txt`, Similarity Score: 0.7266\n","*   Document Pair: `doc1_original.txt` - `doc6_modified.txt`, Similarity Score: 0.6749\n","*   Document Pair: `doc2_original.txt` - `doc7_modified.txt`, Similarity Score: 0.5171\n","*   Document Pair: `doc10_paraphrased.txt` - `doc4_original.txt`, Similarity Score: 0.3584\n","*   Document Pair: `doc11_paraphrased.txt` - `doc5_original.txt`, Similarity Score: 0.2600\n","\n","**Interpretation**: Cosine Similarity performed well in identifying documents with significant lexical overlap, as evidenced by the high scores between original and modified documents. The TF-IDF weighting helps emphasize unique and important terms.\n","\n","#### 4.2. Jaccard Similarity (Bag-of-Words)\n","\n","**Description**: Jaccard Similarity measures the ratio of the intersection to the union of two sets of words (after preprocessing). A score of 1.0 means identical word sets, and 0.0 means no common words. It is sensitive to exact lexical matches.\n","\n","**Top 5 Most Similar Document Pairs**:\n","*   Document Pair: `doc3_original.txt` - `doc8_modified.txt`, Similarity Score: 0.7143\n","*   Document Pair: `doc1_original.txt` - `doc6_modified.txt`, Similarity Score: 0.6667\n","*   Document Pair: `doc2_original.txt` - `doc7_modified.txt`, Similarity Score: 0.4211\n","*   Document Pair: `doc10_paraphrased.txt` - `doc4_original.txt`, Similarity Score: 0.2778\n","*   Document Pair: `doc12_new.txt` - `doc9_paraphrased.txt`, Similarity Score: 0.2500\n","\n","**Interpretation**: Similar to Cosine Similarity, Jaccard Similarity effectively detected direct textual copying and highly similar rephrased content due to its reliance on the exact shared vocabulary.\n","\n","#### 4.3. WordNet Semantic Similarity\n","\n","**Description**: WordNet Semantic Similarity (using path similarity) measures the shortest path between concepts (synsets) in the WordNet hierarchy. This method aims to capture conceptual relationships beyond exact word matches. Scores range from 0.0 (no semantic path) to 1.0 (high semantic relatedness).\n","\n","**Top 5 Most Similar Document Pairs**:\n","*   Document Pair: `doc1_original.txt` - `doc6_modified.txt`, Similarity Score: 0.2665\n","*   Document Pair: `doc10_paraphrased.txt` - `doc4_original.txt`, Similarity Score: 0.2611\n","*   Document Pair: `doc10_paraphrased.txt` - `doc1_original.txt`, Similarity Score: 0.2507\n","*   Document Pair: `doc10_paraphrased.txt` - `doc6_modified.txt`, Similarity Score: 0.2497\n","*   Document Pair: `doc14_new.txt` - `doc6_modified.txt`, Similarity Score: 0.2493\n","\n","**Interpretation**: WordNet similarity yielded generally lower absolute scores compared to the other two metrics. However, it demonstrated the ability to find conceptual links, even between documents with minimal lexical overlap, by leveraging semantic relationships in WordNet.\n","\n","### 5. Comparative Analysis\n","\n","*   **Which similarity metric detected copying best?**\n","    Cosine Similarity (TF-IDF) and Jaccard Similarity proved most effective at detecting direct textual copying or highly lexical similarities. For instance, both metrics assigned very high scores to the `_original.txt` and `_modified.txt` pairs, such as `doc3_original.txt` and `doc8_modified.txt` (Cosine: 0.7266, Jaccard: 0.7143). These high scores accurately reflect that the modified documents were essentially rephrased versions of the originals, maintaining significant lexical overlap.\n","\n","*   **When did Jaccard fail?**\n","    Jaccard Similarity's efficacy is directly tied to the exact lexical overlap between documents. It didn't necessarily 'fail' but showed its limitations when documents conveyed similar meanings using a more diverse vocabulary. For instance, with more heavily paraphrased content like `doc10_paraphrased.txt` and `doc4_original.txt`, Jaccard yielded a score of 0.2778, which is lower than the scores for directly modified pairs. This indicates that while the meaning might be similar, the unique word sets had less direct intersection.\n","\n","*   **When did WordNet help?**\n","    WordNet Semantic Similarity is designed to capture conceptual relationships even when direct lexical overlap is minimal. While its absolute scores were generally lower for the 'modified' and 'paraphrased' documents compared to Cosine and Jaccard (e.g., `doc1_original.txt` - `doc6_modified.txt` at 0.2665), it could be particularly helpful in scenarios where synonyms or semantically related terms are used. It can identify thematic connections, such as between `doc10_paraphrased.txt` and `doc1_original.txt` (0.2507), by linking terms like 'student performance' and 'learning' through their underlying semantic networks, even if the phrasing differs considerably.\n","\n","*   **Any false positives?**\n","    Based on the top 5 results for each metric, there were no clear 'false positives' where highly dissimilar documents received high similarity scores. The top pairs identified by Cosine and Jaccard were genuinely lexically similar (original vs. modified versions). WordNet's top scores were relatively modest (around 0.25-0.26), meaning it wasn't incorrectly flagging unrelated documents as highly similar. However, the interpretation of these lower WordNet scores as 'strong matches' depends on the specific application's threshold for semantic similarity.\n","\n","### 6. Conclusion\n","\n","Each text similarity metric has its unique strengths and is best suited for different types of analysis:\n","\n","*   **Cosine Similarity (TF-IDF)** is excellent for identifying documents that share a significant proportion of important terms. It balances term frequency within a document against its rarity across the corpus, making it robust for detecting rephrasing with varied word usage. It is generally a strong performer for both direct copying and somewhat modified content.\n","\n","*   **Jaccard Similarity** is ideal for situations where exact lexical overlap is the primary criterion for similarity, such as detecting plagiarism or direct duplication. Its simplicity makes it easy to interpret, but its sensitivity to vocabulary differences means it struggles with semantically similar but lexically distinct content.\n","\n","*   **WordNet Semantic Similarity** shines when the goal is to find conceptual relatedness, even if documents use different words to express similar ideas (e.g., through synonyms or hypernyms). While its absolute scores can be lower and harder to interpret without a domain-specific threshold, it provides a valuable tool for understanding the underlying meaning.\n","\n","In summary, for detecting direct \"copying,\" Cosine and Jaccard Similarity are highly effective. For capturing broader semantic connections where specific word choice varies, WordNet Semantic Similarity provides complementary insights. The choice of metric depends heavily on the specific definition of \"similarity\" relevant to the task at hand."]},{"cell_type":"markdown","metadata":{"id":"986daeed"},"source":["## Summary:\n","\n","### Q&A\n","\n","*   **Which similarity metric detected copying best?**\n","    Both Cosine Similarity (TF-IDF) and Jaccard Similarity were most effective at detecting direct textual copying or highly lexical similarities. For instance, both metrics assigned very high scores to the `doc3_original.txt` and `doc8_modified.txt` pair (Cosine: 0.7266, Jaccard: 0.7143).\n","\n","*   **When did Jaccard fail?**\n","    Jaccard Similarity showed limitations when documents conveyed similar meanings using a more diverse vocabulary, such as with more heavily paraphrased content. For example, the similarity between `doc10_paraphrased.txt` and `doc4_original.txt` was 0.2778, indicating less direct lexical intersection despite potential semantic similarity.\n","\n","*   **When did WordNet help?**\n","    WordNet Semantic Similarity helped capture conceptual relationships even with minimal direct lexical overlap. While its absolute scores were generally lower (e.g., `doc1_original.txt` - `doc6_modified.txt` at 0.2665), it could identify thematic connections by linking terms through their underlying semantic networks.\n","\n","*   **Were there any false positives?**\n","    Based on the top 5 results for each metric, there were no clear \"false positives\" where highly dissimilar documents received high similarity scores. The top pairs identified were genuinely related either lexically or semantically.\n","\n","### Data Analysis Key Findings\n","\n","*   **Preprocessing**: All text underwent lowercasing, punctuation removal, tokenization, and stopword removal. WordNet Semantic Similarity also included lemmatization.\n","*   **Cosine Similarity (TF-IDF)**: Demonstrated strong performance in identifying documents with significant lexical overlap, such as `doc3_original.txt` and `doc8_modified.txt` with a score of 0.7266, indicating its effectiveness for rephrased content.\n","*   **Jaccard Similarity**: Effectively detected direct textual copying and highly similar rephrased content, yielding a score of 0.7143 for `doc3_original.txt` and `doc8_modified.txt`, similar to Cosine Similarity.\n","*   **WordNet Semantic Similarity**: While yielding generally lower absolute scores (e.g., `doc1_original.txt` - `doc6_modified.txt` at 0.2665), it effectively identified conceptual links between documents, such as `doc10_paraphrased.txt` and `doc1_original.txt` at 0.2507, even with minimal lexical overlap.\n","*   **Metric Comparison**: Cosine and Jaccard were best for detecting direct copying and high lexical similarity, while WordNet was valuable for uncovering broader semantic connections.\n","*   **No False Positives**: No clear false positives were observed among the top 5 similar document pairs for any of the metrics, suggesting reliable detection within their respective strengths.\n","\n","### Insights or Next Steps\n","\n","*   The choice of text similarity metric should be guided by the specific definition of \"similarity\" required for the task; Cosine and Jaccard are suitable for lexical overlap detection, while WordNet is better for semantic relatedness.\n","*   Future analysis could explore hybrid approaches, combining lexical and semantic metrics, or investigate the impact of domain-specific ontologies on semantic similarity to potentially improve score interpretation and relevance.\n"]}]}