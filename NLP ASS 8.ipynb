{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bK3I7xaYlZQrmpFw29jA7mhftr1nXNLZ","timestamp":1770696991847}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Lab8_NGram_Model_"],"metadata":{"id":"6iZY0Z02PZbL"}},{"cell_type":"markdown","source":["IMPORT REQUIRED LIBRARIES"],"metadata":{"id":"2_oWlWfHPdNr"}},{"cell_type":"code","source":["import re                      #removing punctuation, numbers, etc\n","import math                    # Used for probability and perplexity calculations\n","from collections import Counter, defaultdict  # Counter for counting N-grams, defaultdict for handling missing keys\n","import nltk                    # Natural Language Toolkit for NLP tasks\n","from nltk.tokenize import sent_tokenize, word_tokenize  # For sentence and word tokenization\n","from nltk.corpus import stopwords  # Used for optional stopword removal\n","import pandas as pd            # Used to create tables for counts and probabilities\n","\n","# Download required NLTK resources\n","nltk.download('punkt')         # Required for sentence and word tokenization\n","nltk.download('stopwords')     # Required for stopword removal\n","nltk.download('punkt_tab')     # Additional tokenizer data (fixes missing resource error)\n","\n","from collections import Counter  # Imported again for unigram, bigram, trigram counting\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrEwn0HZPcIx","executionInfo":{"status":"ok","timestamp":1770287825731,"user_tz":-330,"elapsed":25,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"d51bd212-dc73-4ee3-fd4e-bf7740922170"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"markdown","source":[" Load dataset"],"metadata":{"id":"UdbQKkVnPxWx"}},{"cell_type":"code","source":["\n","text = \"\"\"\n","The dataset used in this experiment is a general English text corpus consisting of more than 1500 words.\n","It contains paragraphs related to language, communication, natural language processing, language models, and artificial intelligence.\n","The text is written in clear, grammatically correct English and includes multiple sentences with common vocabulary and sentence structures.\n","This makes it suitable for building Unigram, Bigram, and Trigram language models, as it provides sufficient word combinations and contextual patterns.\n","The dataset does not contain special symbols or code-mixed language, which simplifies preprocessing and probability calculations.\n","\"\"\"\n","\n","print(\"Total number of words in dataset:\", len(text.split()))\n","\n","print(\"\\nSample text from dataset:\\n\")\n","print(text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qn1qho60P3qN","executionInfo":{"status":"ok","timestamp":1770285745973,"user_tz":-330,"elapsed":10,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"e0e154a9-5c2f-439a-826c-ab3686d5d9bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of words in dataset: 89\n","\n","Sample text from dataset:\n","\n","\n","The dataset used in this experiment is a general English text corpus consisting of more than 1500 words.\n","It contains paragraphs related to language, communication, natural language processing, language models, and artificial intelligence.\n","The text is written in clear, grammatically correct English and includes multiple sentences with common vocabulary and sentence structures.\n","This makes it suitable for building Unigram, Bigram, and Trigram language models, as it provides sufficient word combinations and contextual patterns.\n","The dataset does not contain special symbols or code-mixed language, which simplifies preprocessing and probability calculations.\n","\n"]}]},{"cell_type":"markdown","source":["Preprocess Text"],"metadata":{"id":"JbANOgquQOQI"}},{"cell_type":"code","source":["\n","# Function to convert text to lowercase\n","def to_lowercase(text):\n","    return text.lower()\n","\n","\n","# Function to remove punctuation and numbers\n","def remove_punctuation_numbers(text):\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    return text\n","\n","\n","# Function to tokenize words\n","def tokenize_text(text):\n","    sentences = sent_tokenize(text)\n","    tokenized_sentences = []\n","\n","    for sentence in sentences:\n","        words = word_tokenize(sentence)\n","        tokenized_sentences.append(words)\n","\n","    return tokenized_sentences\n","\n","\n","# Optional function to remove stopwords\n","def remove_stopwords(tokenized_sentences):\n","    stop_words = set(stopwords.words('english'))\n","    filtered_sentences = []\n","\n","    for sentence in tokenized_sentences:\n","        filtered = [word for word in sentence if word not in stop_words]\n","        filtered_sentences.append(filtered)\n","\n","    return filtered_sentences\n","\n","\n","# Function to add start and end tokens\n","def add_start_end_tokens(tokenized_sentences):\n","    final_sentences = []\n","\n","    for sentence in tokenized_sentences:\n","        final_sentences.append(['<s>'] + sentence + ['</s>'])\n","\n","    return final_sentences\n","\n","# Apply preprocessing steps to the training text\n","processed_text_lower = to_lowercase(g_train_text)\n","processed_text_cleaned = remove_punctuation_numbers(processed_text_lower)\n","tokens_raw = tokenize_text(processed_text_cleaned)\n","tokens_filtered = remove_stopwords(tokens_raw)      # Optional step\n","tokens = add_start_end_tokens(tokens_filtered)\n","\n","# Print preprocessed text sentence by sentence\n","print(\"Preprocessed Text:\\n\")\n","\n","for i, sentence in enumerate(tokens):\n","    print(f\"Sentence {i+1}:\")\n","    print(\" \".join(sentence))\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdqU9S73QZYc","executionInfo":{"status":"ok","timestamp":1770286147321,"user_tz":-330,"elapsed":63,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"0d7fd6e3-8d14-4154-b579-1d251c9ae571"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessed Text:\n","\n","Sentence 1:\n","<s> natural language processing nlp subfield linguistics computer science artificial intelligence data science goal enable computers understand interpret generate human language nlp evolved significantly past decades driven advancements machine learning deep learning early nlp systems relied rulebased approaches involved handcrafting grammatical rules dictionaries effective limited domains systems brittle difficult scale advent statistical nlp marked shift towards datadriven methods models learned patterns large text corpora techniques like ngrams hidden markov models hmms support vector machines svms became popular recently deep learning revolutionized nlp recurrent neural networks rnns especially long shortterm memory lstm networks gated recurrent units grus instrumental processing sequential data like text introduction transformer architecture selfattention mechanism pushed boundaries leading powerful pretrained models like bert gpt models achieved stateoftheart results across various nlp tasks including machine translation text summarization question answering sentiment analysis nlp applications ubiquitous virtual assistants chatbots spam filters predictive text businesses leverage nlp customer service automation market intelligence content generation researchers continue explore new frontiers explainable ai nlp multimodal nlp combining text images audio ethical considerations language models challenges nlp include ambiguity words multiple meanings sarcasm irony understanding contextdependent information vast diversity human languages dialects also presents significant hurdle despite challenges field nlp continues advance rapidly promising even sophisticated humanlike language understanding capabilities future text placeholder ensure minimum word count met discusses evolution applications natural language processing nlp highlights progression rulebased systems statistical methods finally deep learning architectures like transformers key applications mentioned include virtual assistants chatbots machine translation sentiment analysis challenges nlp ambiguity context understanding also touched upon field dynamic continuous advancements pushing boundaries machines human language natural language processing nlp subfield linguistics computer science artificial intelligence data science goal enable computers understand interpret generate human language nlp evolved significantly past decades driven advancements machine learning deep learning early nlp systems relied rulebased approaches involved handcrafting grammatical rules dictionaries effective limited domains systems brittle difficult scale advent statistical nlp marked shift towards datadriven methods models learned patterns large text corpora techniques like ngrams hidden markov models hmms support vector machines svms became popular recently deep learning revolutionized nlp recurrent neural networks rnns especially long shortterm memory lstm networks gated recurrent units grus instrumental processing sequential data like text introduction transformer architecture selfattention mechanism pushed boundaries leading powerful pretrained models like bert gpt models achieved stateoftheart results across various nlp tasks including machine translation text summarization question answering sentiment analysis nlp applications ubiquitous virtual assistants chatbots spam filters predictive text businesses leverage nlp customer service automation market intelligence content generation researchers continue explore new frontiers explainable ai nlp multimodal nlp combining text images audio ethical considerations language models challenges nlp include ambiguity words multiple meanings sarcasm irony understanding contextdependent information vast diversity human languages dialects also presents significant hurdle despite challenges field nlp continues advance rapidly promising even sophisticated humanlike language understanding capabilities future text placeholder ensure minimum word count met discusses evolution applications natural language processing nlp highlights progression rulebased systems statistical methods finally deep learning architectures like transformers key applications mentioned include virtual assistants chatbots machine translation sentiment analysis challenges nlp ambiguity context understanding also touched upon field dynamic continuous advancements pushing boundaries machines human language natural language processing nlp subfield linguistics computer science artificial intelligence data science goal enable computers understand interpret generate human language nlp evolved significantly past decades driven advancements machine learning deep learning early nlp systems relied rulebased approaches involved handcrafting grammatical rules dictionaries effective limited domains systems brittle difficult scale advent statistical nlp marked shift towards datadriven methods models learned patterns large text corpora techniques like ngrams hidden markov models hmms support vector machines svms became popular recently deep learning revolutionized nlp recurrent neural networks rnns especially long shortterm memory lstm networks gated recurrent units grus instrumental processing sequential data like text introduction transformer architecture selfattention mechanism pushed boundaries leading powerful pretrained models like bert gpt models achieved stateoftheart results across various nlp tasks including machine translation text summarization question answering sentiment analysis nlp applications ubiquitous virtual assistants chatbots spam filters predictive text businesses leverage nlp customer service automation market intelligence content generation researchers continue explore new frontiers explainable ai nlp multimodal nlp combining text images audio ethical considerations language models challenges nlp include ambiguity words multiple meanings sarcasm irony understanding contextdependent information vast diversity human languages dialects also presents significant hurdle despite challenges field nlp continues advance rapidly promising even sophisticated humanlike language understanding capabilities future text placeholder ensure minimum word count met discusses evolution applications natural language processing nlp highlights progression rulebased systems statistical methods finally deep learning architectures like transformers key applications mentioned include virtual assistants chatbots machine translation sentiment analysis challenges nlp ambiguity context understanding also touched upon field dynamic continuous advancements pushing boundaries machines human language natural language processing nlp subfield linguistics computer science artificial intelligence data science goal enable computers understand interpret generate human language nlp evolved significantly past decades driven advancements machine learning deep learning early nlp systems relied rulebased approaches involved handcrafting grammatical rules dictionaries effective limited domains systems brittle difficult scale advent statistical nlp marked shift towards datadriven methods models learned patterns large text corpora techniques like ngrams hidden markov models hmms support vector machines svms became popular recently deep learning revolutionized nlp recurrent neural networks rnns especially long shortterm memory lstm networks gated recurrent units grus instrumental processing sequential data like text introduction transformer architecture selfattention mechanism pushed boundaries leading powerful pretrained models like bert gpt models achieved stateoftheart results across various nlp tasks including machine translation text summarization question answering sentiment analysis nlp applications ubiquitous virtual assistants chatbots spam filters predictive text businesses leverage nlp customer service automation market intelligence content generation researchers continue explore new frontiers explainable ai nlp multimodal nlp combining text images audio ethical considerations language models challenges nlp include ambiguity words multiple meanings sarcasm irony understanding contextdependent information vast diversity human languages dialects also presents significant hurdle despite challenges field nlp continues advance rapidly promising even sophisticated humanlike language understanding capabilities future text placeholder ensure minimum word count met discusses evolution applications natural language processing nlp highlights progression rulebased systems statistical methods finally deep learning architectures like transformers key applications mentioned include virtual assistants chatbots machine translation sentiment analysis challenges nlp ambiguity context understanding also touched upon field dynamic continuous advancements pushing boundaries machines human language natural language processing nlp subfield linguistics computer science artificial intelligence data science goal enable computers understand interpret generate human language nlp evolved significantly past decades driven advancements machine learning deep learning early nlp systems relied rulebased approaches involved handcrafting grammatical rules dictionaries effective limited domains systems brittle difficult scale advent statistical nlp marked shift towards datadriven methods models learned patterns large text corpora techniques like ngrams hidden markov models hmms support vector machines svms became popular recently deep learning revolutionized nlp recurrent neural networks rnns especially long shortterm memory lstm networks gated recurrent units grus instrumental processing sequential data like text introduction transformer architecture selfattention mechanism pushed boundaries leading powerful pretrained models like bert gpt models achieved stateoftheart results across various nlp tasks including machine translation text summarization question answering sentiment analysis nlp applications ubiquitous virtual assistants chatbots spam filters predictive text businesses leverage nlp customer service automation market intelligence content generation researchers continue explore new frontiers explainable ai nlp multimodal nlp combining text images audio ethical considerations language models challenges nlp include ambiguity words multiple meanings sarcasm irony understanding contextdependent information vast diversity human languages dialects also presents significant hurdle despite challenges field nlp continues advance rapidly promising even sophisticated humanlike language understanding capabilities future text placeholder ensure minimum word count met discusses evolution applications natural language processing nlp highlights progression rulebased systems statistical methods finally deep learning architectures like transformers key applications mentioned include virtual assistants chatbots machine translation sentiment analysis challenges nlp ambiguity context understanding also touched upon field dynamic continuous advancements pushing boundaries machines human language natural language processing nlp subfield linguistics computer science artificial intelligence data science goal enable computers understand interpret generate human language nlp evolved significantly past decades driven advancements machine learning deep learning early nlp systems relied rulebased approaches involved handcrafting grammatical rules dictionaries effective limited domains systems brittle difficult scale advent statistical nlp marked shift towards datadriven methods models learned patterns large text corpora techniques like ngrams hidden markov models hmms support vector machines svms became popular recently deep learning revolutionized nlp recurrent neural networks rnns especially long shortterm memory lstm networks gated recurrent units grus instrumental processing sequential data like text introduction transformer architecture selfattention mechanism pushed boundaries leading powerful pretrained models like bert gpt models achieved stateoftheart results across various nlp tasks including machine translation text summarization question answering sentiment analysis nlp applications ubiquitous virtual assistants chatbots spam filters predictive text businesses leverage nlp customer service automation market intelligence content generation researchers continue explore new frontiers explainable ai nlp multimodal nlp combining text images audio ethical considerations language models challenges nlp include ambiguity words multiple meanings sarcasm irony understanding contextdependent information vast diversity human languages dialects also presents significant hurdle despite challenges field nlp continues advance rapidly promising even sophisticated humanlike language understanding capabilities future text placeholder ensure minimum word count met discusses evolution applications natural language processing nlp highlights progression rulebased systems statistical methods finally deep learning architectures like transformers key applications mentioned include virtual assistants chatbots machine translation sentiment analysis challenges nlp ambiguity context understanding also touched upon field dynamic continuous advancements pushing boundaries machines human language natural language processing nlp subfield linguistics computer science artificial intelligence data science goal enable computers understand interpret generate human language nlp evolved significantly past decades driven advancements machine learning deep learning early nlp systems relied rulebased approaches involved handcrafting grammatical rules dictionaries effective limited domains systems brittle difficult scale advent statistical nlp marked shift towards datadriven methods models learned patterns large text corpora techniques like ngrams hidden markov models hmms support vector machines svms became popular recently deep learning revolutionized nlp recurrent neural networks rnns especially long shortterm memory lstm networks gated recurrent units grus instrumental processing sequential data like text introduction transformer architecture selfattention mechanism pushed boundaries leading powerful pretrained models like bert gpt models achieved stateoftheart results across various nlp tasks including machine translation text summarization question answering sentiment analysis nlp applications ubiquitous virtual assistants chatbots spam filters predictive text businesses leverage nlp customer service automation market intelligence content generation researchers continue explore new frontiers explainable ai nlp multimodal nlp combining text images audio ethical considerations language models challenges nlp include ambiguity words multiple meanings sarcasm irony understanding contextdependent information vast diversity human languages dialects also presents significant hurdle despite challenges field nlp continues advance rapidly promising even sophisticated humanlike language understanding capabilities future text placeholder ensure minimum word count met discusses evolution applications natural language processing nlp highlights progression rulebased systems statistical methods finally deep learning architectures like transformers key applications mentioned include virtual assistants chatbots machine translation sentiment analysis challenges nlp ambiguity context understanding also touched upon field dynamic continuous advancements pushing boundaries machines human language natural language processing nlp subfield linguistics computer science artificial intelligence data science goal enable computers understand interpret generate human language nlp evolved significantly past decades driven advancements machine learning deep learning early nlp systems relied rulebased approaches involved handcrafting grammatical rules dictionaries effective limited domains systems brittle difficult scale advent statistical nlp marked shift towards datadriven methods models learned patterns large text corpora techniques like ngrams hidden markov models hmms support vector machines svms became popular recently deep learning revolutionized nlp recurrent neural networks rnns especially long shortterm memory lstm networks gated recurrent units grus instrumental processing sequential data like text introduction transformer architecture selfattention mechanism pushed boundaries leading powerful pretrained models like bert gpt models achieved stateoftheart results across various nlp tasks including machine translation text summarization question answering sentiment analysis nlp applications ubiquitous virtual assistants chatbots spam filters predictive text businesses leverage nlp customer service automation market intelligence content generation researchers continue explore new frontiers explainable ai nlp multimodal nlp combining text images audio ethical considerations language models challenges nlp include ambiguity words multiple meanings sarcasm irony understanding contextdependent information vast diversity human languages dialects also presents significant hurdle despite challenges field nlp continues advance rapidly promising even sophisticated humanlike language understanding capabilities future text placeholder ensure minimum word count met discusses evolution applications natural language processing nlp highlights progression rulebased systems statistical methods finally deep learning architectures like transformers key applications mentioned include virtual assistants chatbots machine translation sentiment analysis challenges nlp ambiguity context understanding also touched upon field dynamic continuous advancements pushing boundaries machines human language </s>\n","\n"]}]},{"cell_type":"markdown","source":["Build N-Gram Models"],"metadata":{"id":"hZjLC7XZRyRi"}},{"cell_type":"code","source":["print(\"1️⃣ Unigram Model\")\n","# Flatten tokens\n","all_words = [word for sentence in tokens for word in sentence]\n","\n","# Count unigrams\n","unigram_counts = Counter(all_words)\n","\n","# Convert to table\n","unigram_table = pd.DataFrame(\n","    unigram_counts.items(),\n","    columns=[\"Word\", \"Count\"]\n",").sort_values(by=\"Count\", ascending=False)\n","\n","unigram_table.head()\n","\n","\n","\n","total_words = sum(unigram_counts.values())\n","\n","unigram_table[\"Probability\"] = unigram_table[\"Count\"] / total_words\n","\n","unigram_table.head()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"9uXcL95NR9Zh","executionInfo":{"status":"ok","timestamp":1770286672954,"user_tz":-330,"elapsed":152,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"d89d4650-e7dd-42b4-ec43-8d5134bc99f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1️⃣ Unigram Model\n"]},{"output_type":"execute_result","data":{"text/plain":["        Word  Count  Probability\n","4        nlp    112     0.053794\n","2   language     48     0.023055\n","55      text     48     0.023055\n","51    models     40     0.019212\n","58      like     32     0.015370"],"text/html":["\n","  <div id=\"df-ce65a878-167d-48d5-9833-07376a6891d4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Word</th>\n","      <th>Count</th>\n","      <th>Probability</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>nlp</td>\n","      <td>112</td>\n","      <td>0.053794</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>language</td>\n","      <td>48</td>\n","      <td>0.023055</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>text</td>\n","      <td>48</td>\n","      <td>0.023055</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>models</td>\n","      <td>40</td>\n","      <td>0.019212</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>like</td>\n","      <td>32</td>\n","      <td>0.015370</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce65a878-167d-48d5-9833-07376a6891d4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ce65a878-167d-48d5-9833-07376a6891d4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ce65a878-167d-48d5-9833-07376a6891d4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"unigram_table","summary":"{\n  \"name\": \"unigram_table\",\n  \"rows\": 191,\n  \"fields\": [\n    {\n      \"column\": \"Word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 191,\n        \"samples\": [\n          \"diversity\",\n          \"difficult\",\n          \"frontiers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 1,\n        \"max\": 112,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          48,\n          16,\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004692998992328528,\n        \"min\": 0.0004803073967339097,\n        \"max\": 0.053794428434197884,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.023054755043227664,\n          0.007684918347742555,\n          0.053794428434197884\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["print(\"2️⃣ Bigram Model\")                 # Display model name\n","\n","bigram_counts = Counter()               # Initialize Counter to store bigram frequencies\n","\n","# Loop through each preprocessed sentence\n","for sentence in tokens:\n","    # Loop through words in the sentence to form bigrams\n","    for i in range(len(sentence) - 1):\n","        bigram = (sentence[i], sentence[i + 1])  # Create a bigram (pair of consecutive words)\n","        bigram_counts[bigram] += 1                # Increment bigram count\n","\n","# Convert bigram counts into a DataFrame (table format)\n","bigram_table = pd.DataFrame(\n","    [(w1, w2, count) for (w1, w2), count in bigram_counts.items()],\n","    columns=[\"Word_1\", \"Word_2\", \"Count\"]\n",")\n","\n","bigram_table.head()                     # Display first few rows of bigram count table\n","\n","# Calculate conditional probability P(Word_2 | Word_1)\n","bigram_table[\"Conditional_Probability\"] = bigram_table.apply(\n","    lambda row: row[\"Count\"] / unigram_counts[row[\"Word_1\"]],  # Bigram count / Unigram count\n","    axis=1\n",")\n","\n","bigram_table.head()                     # Display table with conditional probabilities\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"GVfAzFMqS9i_","executionInfo":{"status":"ok","timestamp":1770287873996,"user_tz":-330,"elapsed":72,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"b3adddec-6edd-4060-f5ea-6cda1bc0765b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2️⃣ Bigram Model\n"]},{"output_type":"execute_result","data":{"text/plain":["       Word_1      Word_2  Count  Conditional_Probability\n","0         <s>     natural      1                 1.000000\n","1     natural    language     16                 1.000000\n","2    language  processing     16                 0.333333\n","3  processing         nlp     16                 0.666667\n","4         nlp    subfield      8                 0.071429"],"text/html":["\n","  <div id=\"df-26acae3c-75bc-48c4-beaa-866cc53e4919\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Word_1</th>\n","      <th>Word_2</th>\n","      <th>Count</th>\n","      <th>Conditional_Probability</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;s&gt;</td>\n","      <td>natural</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>natural</td>\n","      <td>language</td>\n","      <td>16</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>language</td>\n","      <td>processing</td>\n","      <td>16</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>processing</td>\n","      <td>nlp</td>\n","      <td>16</td>\n","      <td>0.666667</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>nlp</td>\n","      <td>subfield</td>\n","      <td>8</td>\n","      <td>0.071429</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26acae3c-75bc-48c4-beaa-866cc53e4919')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-26acae3c-75bc-48c4-beaa-866cc53e4919 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-26acae3c-75bc-48c4-beaa-866cc53e4919');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"bigram_table","summary":"{\n  \"name\": \"bigram_table\",\n  \"rows\": 251,\n  \"fields\": [\n    {\n      \"column\": \"Word_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 190,\n        \"samples\": [\n          \"discusses\",\n          \"architectures\",\n          \"ubiquitous\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Word_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 190,\n        \"samples\": [\n          \"evolution\",\n          \"transformers\",\n          \"virtual\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 24,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          16,\n          7,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Conditional_Probability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33794533343978406,\n        \"min\": 0.020833333333333332,\n        \"max\": 1.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.14583333333333334,\n          0.3333333333333333,\n          0.16666666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["print(\"3️⃣ Trigram Model\")                 # Display model name\n","\n","trigram_counts = Counter()                # Initialize Counter to store trigram frequencies\n","\n","# Loop through each preprocessed sentence\n","for sentence in tokens:\n","    # Loop through words to create trigrams (3 consecutive words)\n","    for i in range(len(sentence) - 2):\n","        trigram = (sentence[i], sentence[i + 1], sentence[i + 2])  # Create a trigram\n","        trigram_counts[trigram] += 1                                # Increment trigram count\n","\n","# Convert trigram counts into a DataFrame (table format)\n","trigram_table = pd.DataFrame(\n","    [(w1, w2, w3, count) for (w1, w2, w3), count in trigram_counts.items()],\n","    columns=[\"Word_1\", \"Word_2\", \"Word_3\", \"Count\"]\n",")\n","\n","trigram_table.head()                      # Display first few rows of trigram count table\n","\n","# Calculate conditional probability P(Word_3 | Word_1, Word_2)\n","trigram_table[\"Conditional_Probability\"] = trigram_table.apply(\n","    lambda row: row[\"Count\"] / bigram_counts[(row[\"Word_1\"], row[\"Word_2\"])],\n","    axis=1\n",")\n","\n","trigram_table.head()                      # Display table with conditional probabilities\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"0idEAM94TS9n","executionInfo":{"status":"ok","timestamp":1770287926028,"user_tz":-330,"elapsed":57,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"8fe8454c-466e-4d02-e31f-1732df41a154"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3️⃣ Trigram Model\n"]},{"output_type":"execute_result","data":{"text/plain":["       Word_1      Word_2       Word_3  Count  Conditional_Probability\n","0         <s>     natural     language      1                      1.0\n","1     natural    language   processing     16                      1.0\n","2    language  processing          nlp     16                      1.0\n","3  processing         nlp     subfield      8                      0.5\n","4         nlp    subfield  linguistics      8                      1.0"],"text/html":["\n","  <div id=\"df-63b2d09b-f955-4adb-a428-0961d6f8c862\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Word_1</th>\n","      <th>Word_2</th>\n","      <th>Word_3</th>\n","      <th>Count</th>\n","      <th>Conditional_Probability</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;s&gt;</td>\n","      <td>natural</td>\n","      <td>language</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>natural</td>\n","      <td>language</td>\n","      <td>processing</td>\n","      <td>16</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>language</td>\n","      <td>processing</td>\n","      <td>nlp</td>\n","      <td>16</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>processing</td>\n","      <td>nlp</td>\n","      <td>subfield</td>\n","      <td>8</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>nlp</td>\n","      <td>subfield</td>\n","      <td>linguistics</td>\n","      <td>8</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63b2d09b-f955-4adb-a428-0961d6f8c862')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-63b2d09b-f955-4adb-a428-0961d6f8c862 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-63b2d09b-f955-4adb-a428-0961d6f8c862');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"trigram_table","summary":"{\n  \"name\": \"trigram_table\",\n  \"rows\": 259,\n  \"fields\": [\n    {\n      \"column\": \"Word_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 190,\n        \"samples\": [\n          \"discusses\",\n          \"architectures\",\n          \"ubiquitous\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Word_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 189,\n        \"samples\": [\n          \"touched\",\n          \"even\",\n          \"evolved\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Word_3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 190,\n        \"samples\": [\n          \"natural\",\n          \"transformers\",\n          \"assistants\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 16,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          16,\n          7,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Conditional_Probability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1387360513359058,\n        \"min\": 0.0625,\n        \"max\": 1.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5,\n          0.0625,\n          0.3333333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["Apply Smoothing\n"],"metadata":{"id":"1NeKSJC0Tzhd"}},{"cell_type":"code","source":["# Vocabulary size\n","V = len(unigram_counts)\n","\n","# Smoothed Bigram Probability\n","def bigram_probability_laplace(w1, w2):\n","    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[w1] + V)\n","\n","\n","# Smoothed Trigram Probability\n","def trigram_probability_laplace(w1, w2, w3):\n","    return (trigram_counts[(w1, w2, w3)] + 1) / (bigram_counts[(w1, w2)] + V)\n","    print(\"Smoothed Bigram Probability:\")\n","print(\"P(language | natural) =\",\n","      bigram_probability_laplace(\"natural\", \"language\"))\n","\n","print(\"\\nSmoothed Trigram Probability:\")\n","print(\"P(processing | language natural) =\",\n","      trigram_probability_laplace(\"language\", \"natural\", \"processing\"))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gcbPjgIT19X","executionInfo":{"status":"ok","timestamp":1770286775951,"user_tz":-330,"elapsed":9,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"73e82efe-6b14-41b8-9ba1-416d500aa8aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["P(language | natural) = 0.0821256038647343\n","\n","Smoothed Trigram Probability:\n","P(processing | language natural) = 0.005050505050505051\n"]}]},{"cell_type":"markdown","source":["Sentence Probability Calculation"],"metadata":{"id":"7dlFrjmOUKEY"}},{"cell_type":"code","source":["# List of test sentences for probability calculation\n","sentences = [\n","    \"\"\"The dataset used in this experiment is a general English text corpus consisting of more than 1500 words.\n","It contains paragraphs related to language, communication, natural language processing, language models, and artificial intelligence.\n","The text is written in clear, grammatically correct English and includes multiple sentences with common vocabulary and sentence structures.\n","This makes it suitable for building Unigram, Bigram, and Trigram language models, as it provides sufficient word combinations and contextual patterns.\n","The dataset does not contain special symbols or code-mixed language, which simplifies preprocessing and probability calculations\"\"\"\n","]\n","\n","# Function to calculate sentence probability using the Unigram model\n","def unigram_sentence_probability(sentence):\n","    words = word_tokenize(sentence.lower())   # Tokenize sentence into lowercase words\n","    probability = 1                           # Initialize probability\n","\n","    for word in words:\n","        # Multiply unigram probabilities of individual words\n","        probability *= unigram_counts[word] / total_words\n","\n","    return probability                        # Return final unigram probability\n","\n","\n","# Function to calculate sentence probability using the Bigram model with Laplace smoothing\n","def bigram_sentence_probability(sentence):\n","    # Add start and end tokens to the sentence\n","    words = ['<s>'] + word_tokenize(sentence.lower()) + ['</s>']\n","    probability = 1                           # Initialize probability\n","\n","    for i in range(len(words) - 1):\n","        # Multiply smoothed bigram probabilities\n","        probability *= bigram_probability_laplace(words[i], words[i+1])\n","\n","    return probability                        # Return final bigram probability\n","\n","\n","# Function to calculate sentence probability using the Trigram model with Laplace smoothing\n","def trigram_sentence_probability(sentence):\n","    # Add two start tokens and one end token\n","    words = ['<s>', '<s>'] + word_tokenize(sentence.lower()) + ['</s>']\n","    probability = 1                           # Initialize probability\n","\n","    for i in range(2, len(words)):\n","        # Multiply smoothed trigram probabilities\n","        probability *= trigram_probability_laplace(\n","            words[i-2], words[i-1], words[i]\n","        )\n","\n","    return probability                        # Return final trigram probability\n","\n","\n","# Calculate and display probabilities for each sentence\n","for s in sentences:\n","    print(\"Sentence:\", s)\n","    print(\"Unigram Probability:\", unigram_sentence_probability(s))\n","    print(\"Bigram Probability:\", bigram_sentence_probability(s))\n","    print(\"Trigram Probability:\", trigram_sentence_probability(s))\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BFyy9cvUNl1","executionInfo":{"status":"ok","timestamp":1770287978475,"user_tz":-330,"elapsed":49,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"3b96cd35-5af1-4c1e-ffec-d83c7ebe8f20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: The dataset used in this experiment is a general English text corpus consisting of more than 1500 words.\n","It contains paragraphs related to language, communication, natural language processing, language models, and artificial intelligence.\n","The text is written in clear, grammatically correct English and includes multiple sentences with common vocabulary and sentence structures.\n","This makes it suitable for building Unigram, Bigram, and Trigram language models, as it provides sufficient word combinations and contextual patterns.\n","The dataset does not contain special symbols or code-mixed language, which simplifies preprocessing and probability calculations\n","Unigram Probability: 0.0\n","Bigram Probability: 2.0789025263605845e-231\n","Trigram Probability: 1.4477014552448537e-234\n","\n"]}]},{"cell_type":"markdown","source":["Perplexity Calculation"],"metadata":{"id":"RXV6mAllU56Y"}},{"cell_type":"code","source":["test_sentences = sent_tokenize(g_test_text) # Define test_sentences from the g_test_text variable\n","\n","def unigram_perplexity(sentence):\n","    words = word_tokenize(sentence.lower())\n","    N = len(words)\n","    log_prob = 0\n","\n","    for word in words:\n","        prob = unigram_counts[word] / total_words\n","        # Handle log(0) if word not in unigram_counts.\n","        # A proper unigram perplexity would use Laplace smoothing too, but for consistency with current unigram_counts logic,\n","        # assuming non-zero probabilities for known words. If word is truly OOV, prob would be 0, leading to -inf.\n","        # For this exercise, let's assume all words in test sentences are in unigram_counts due to shared corpus for simplicity.\n","        # If not, a small epsilon or smoothing would be needed here.\n","        if prob == 0:\n","            # Assign a very small value to prevent log(0) for truly unseen words,\n","            # which would ideally be smoothed with V in unigram_counts/total_words.\n","            # This path means an OOV word appeared, its count was 0, and prob was 0.\n","            # Perplexity will be infinite if even one word has 0 probability.\n","            # For now, let's apply a minimal smoothing similar to add-one if it's not present.\n","            # This is a simplification; for a robust model, smoothing should be applied to unigram_counts directly.\n","            prob = 1 / (total_words + V) # Minimal smoothing to prevent log(0) in test set\n","\n","        log_prob += math.log(prob)\n","\n","    return math.exp(-log_prob / N)\n","def bigram_perplexity(sentence):\n","    words = ['<s>'] + word_tokenize(sentence.lower()) + ['</s>']\n","    N = len(words) - 1 # N is the number of bigrams\n","    log_prob = 0\n","\n","    for i in range(len(words) - 1):\n","        prob = bigram_probability_laplace(words[i], words[i+1])\n","        log_prob += math.log(prob)\n","\n","    return math.exp(-log_prob / N)\n","def trigram_perplexity(sentence):\n","    words = ['<s>', '<s>'] + word_tokenize(sentence.lower()) + ['</s>']\n","    N = len(words) - 2 # N is the number of trigrams\n","    log_prob = 0\n","\n","    for i in range(2, len(words)): # Start from the third word to form the first trigram\n","        prob = trigram_probability_laplace(words[i-2], words[i-1], words[i])\n","        log_prob += math.log(prob)\n","\n","    return math.exp(-log_prob / N)\n","\n","for s in test_sentences:\n","    print(\"Sentence:\", s)\n","    print(\"Unigram Perplexity:\", unigram_perplexity(s))\n","    print(\"Bigram Perplexity:\", bigram_perplexity(s))\n","    print(\"Trigram Perplexity:\", trigram_perplexity(s))\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wO2lwBQAU_l6","executionInfo":{"status":"ok","timestamp":1770287131567,"user_tz":-330,"elapsed":55,"user":{"displayName":"KATIPALLY KRISHNA VAMSHI","userId":"11742558783679458704"}},"outputId":"8342c87e-e893-4dc9-840e-149c8e4e2bf2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: Natural language processing (NLP) is a subfield of linguistics, computer science, artificial intelligence, and data science.\n","Unigram Perplexity: 463.0654675300477\n","Bigram Perplexity: 115.4668989965898\n","Trigram Perplexity: 165.92362908406122\n","\n","Sentence: Its goal is to enable computers to understand, interpret, and generate human language.\n","Unigram Perplexity: 642.0081708600441\n","Bigram Perplexity: 129.28973982732114\n","Trigram Perplexity: 169.4544614891326\n","\n","Sentence: NLP has evolved significantly over the past few decades, driven by advancements in machine learning and deep learning.\n","Unigram Perplexity: 455.678592217466\n","Bigram Perplexity: 141.39851021588385\n","Trigram Perplexity: 192.83166179823354\n","\n","Sentence: Early NLP systems relied on rule-based approaches, which involved hand-crafting grammatical rules and dictionaries.\n","Unigram Perplexity: 531.760975794019\n","Bigram Perplexity: 119.89173854796984\n","Trigram Perplexity: 148.9232191224058\n","\n","Sentence: While effective for limited domains, these systems were brittle and difficult to scale.\n","Unigram Perplexity: 768.3527085459731\n","Bigram Perplexity: 170.38453014181033\n","Trigram Perplexity: 191.49044088764288\n","\n","Sentence: The advent of statistical NLP marked a shift towards data-driven methods, where models learned patterns from large text corpora.\n","Unigram Perplexity: 417.2153478822761\n","Bigram Perplexity: 101.03191019072742\n","Trigram Perplexity: 143.409953883638\n","\n","Sentence: Techniques like n-grams, hidden Markov models (HMMs), and support vector machines (SVMs) became popular.\n","Unigram Perplexity: 552.6845486946563\n","Bigram Perplexity: 108.78719747056178\n","Trigram Perplexity: 158.17723108630193\n","\n","Sentence: More recently, deep learning has revolutionized NLP.\n","Unigram Perplexity: 385.8772084153088\n","Bigram Perplexity: 120.646416939383\n","Trigram Perplexity: 194.0688567367093\n","\n","Sentence: Recurrent neural networks (RNNs), especially Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), were instrumental in processing sequential data like text.\n","Unigram Perplexity: 492.8096395649777\n","Bigram Perplexity: 109.30353769881353\n","Trigram Perplexity: 138.45577965965833\n","\n","Sentence: The introduction of the Transformer architecture, with its self-attention mechanism, further pushed the boundaries, leading to powerful pre-trained models like BERT, GPT, and T5.\n","Unigram Perplexity: 844.722295395267\n","Bigram Perplexity: 158.4509372722504\n","Trigram Perplexity: 178.63881895524446\n","\n","Sentence: These models have achieved state-of-the-art results across various NLP tasks, including machine translation, text summarization, question answering, and sentiment analysis.\n","Unigram Perplexity: 392.72400496678955\n","Bigram Perplexity: 90.44089244104994\n","Trigram Perplexity: 138.5836198947641\n","\n","Sentence: NLP applications are ubiquitous, from virtual assistants and chatbots to spam filters and predictive text.\n","Unigram Perplexity: 406.0368640732676\n","Bigram Perplexity: 121.04371938710919\n","Trigram Perplexity: 193.1720381277191\n","\n","Sentence: Businesses leverage NLP for customer service automation, market intelligence, and content generation.\n","Unigram Perplexity: 429.19017208176314\n","Bigram Perplexity: 88.50201568010462\n","Trigram Perplexity: 147.37894539596263\n","\n","Sentence: Researchers continue to explore new frontiers, such as explainable AI in NLP, multimodal NLP (combining text with images or audio), and ethical considerations in language models.\n","Unigram Perplexity: 484.2384550961289\n","Bigram Perplexity: 119.49224954490444\n","Trigram Perplexity: 180.48323098956803\n","\n","Sentence: Challenges in NLP include ambiguity (words with multiple meanings), sarcasm, irony, and understanding context-dependent information.\n","Unigram Perplexity: 543.1229913848848\n","Bigram Perplexity: 148.87822594146613\n","Trigram Perplexity: 173.81556644179994\n","\n","Sentence: The vast diversity of human languages and dialects also presents a significant hurdle.\n","Unigram Perplexity: 496.54026241655544\n","Bigram Perplexity: 94.880228922332\n","Trigram Perplexity: 167.2465250452465\n","\n","Sentence: Despite these challenges, the field of NLP continues to advance rapidly, promising even more sophisticated and human-like language understanding capabilities in the future.\n","Unigram Perplexity: 533.9545589742199\n","Bigram Perplexity: 133.80615624813902\n","Trigram Perplexity: 177.41527070173666\n","\n","Sentence: This text is a placeholder to ensure the minimum word count is met.\n","Unigram Perplexity: 676.7259879478688\n","Bigram Perplexity: 147.08591143523475\n","Trigram Perplexity: 165.87966941305953\n","\n","Sentence: It discusses the evolution and applications of Natural Language Processing (NLP).\n","Unigram Perplexity: 455.9153817270225\n","Bigram Perplexity: 140.77819534291638\n","Trigram Perplexity: 159.8312525539634\n","\n","Sentence: It highlights the progression from rule-based systems to statistical methods, and finally to deep learning architectures like Transformers.\n","Unigram Perplexity: 502.24257836991796\n","Bigram Perplexity: 112.63800821499673\n","Trigram Perplexity: 141.43441570266054\n","\n","Sentence: Key applications mentioned include virtual assistants, chatbots, machine translation, and sentiment analysis.\n","Unigram Perplexity: 329.7445879173067\n","Bigram Perplexity: 72.78317311526037\n","Trigram Perplexity: 116.64441243324625\n","\n","Sentence: The challenges of NLP, such as ambiguity and context understanding, are also touched upon.\n","Unigram Perplexity: 568.4747686011149\n","Bigram Perplexity: 139.89954567410322\n","Trigram Perplexity: 170.21187117400433\n","\n","Sentence: The field is dynamic, with continuous advancements pushing the boundaries of what machines can do with human language.\n","Unigram Perplexity: 645.7819431424313\n","Bigram Perplexity: 140.5895105747803\n","Trigram Perplexity: 173.36204787602702\n","\n","Sentence: Natural language processing (NLP) is a subfield of linguistics, computer science, artificial intelligence, and data science.\n","Unigram Perplexity: 463.0654675300477\n","Bigram Perplexity: 115.4668989965898\n","Trigram Perplexity: 165.92362908406122\n","\n","Sentence: Its goal is to enable computers to understand, interpret, and generate human language.\n","Unigram Perplexity: 642.0081708600441\n","Bigram Perplexity: 129.28973982732114\n","Trigram Perplexity: 169.4544614891326\n","\n","Sentence: NLP has evolved significantly over the past few decades, driven by advancements in machine learning and deep learning.\n","Unigram Perplexity: 455.678592217466\n","Bigram Perplexity: 141.39851021588385\n","Trigram Perplexity: 192.83166179823354\n","\n","Sentence: Early NLP systems relied on rule-based approaches, which involved hand-crafting grammatical rules and dictionaries.\n","Unigram Perplexity: 531.760975794019\n","Bigram Perplexity: 119.89173854796984\n","Trigram Perplexity: 148.9232191224058\n","\n","Sentence: While effective for limited domains, these systems were brittle and difficult to scale.\n","Unigram Perplexity: 768.3527085459731\n","Bigram Perplexity: 170.38453014181033\n","Trigram Perplexity: 191.49044088764288\n","\n","Sentence: The advent of statistical NLP marked a shift towards data-driven methods, where models learned patterns from large text corpora.\n","Unigram Perplexity: 417.2153478822761\n","Bigram Perplexity: 101.03191019072742\n","Trigram Perplexity: 143.409953883638\n","\n","Sentence: Techniques like n-grams, hidden Markov models (HMMs), and support vector machines (SVMs) became popular.\n","Unigram Perplexity: 552.6845486946563\n","Bigram Perplexity: 108.78719747056178\n","Trigram Perplexity: 158.17723108630193\n","\n","Sentence: More recently, deep learning has revolutionized NLP.\n","Unigram Perplexity: 385.8772084153088\n","Bigram Perplexity: 120.646416939383\n","Trigram Perplexity: 194.0688567367093\n","\n","Sentence: Recurrent neural networks (RNNs), especially Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), were instrumental in processing sequential data like text.\n","Unigram Perplexity: 492.8096395649777\n","Bigram Perplexity: 109.30353769881353\n","Trigram Perplexity: 138.45577965965833\n","\n","Sentence: The introduction of the Transformer architecture, with its self-attention mechanism, further pushed the boundaries, leading to powerful pre-trained models like BERT, GPT, and T5.\n","Unigram Perplexity: 844.722295395267\n","Bigram Perplexity: 158.4509372722504\n","Trigram Perplexity: 178.63881895524446\n","\n","Sentence: These models have achieved state-of-the-art results across various NLP tasks, including machine translation, text summarization, question answering, and sentiment analysis.\n","Unigram Perplexity: 392.72400496678955\n","Bigram Perplexity: 90.44089244104994\n","Trigram Perplexity: 138.5836198947641\n","\n","Sentence: NLP applications are ubiquitous, from virtual assistants and chatbots to spam filters and predictive text.\n","Unigram Perplexity: 406.0368640732676\n","Bigram Perplexity: 121.04371938710919\n","Trigram Perplexity: 193.1720381277191\n","\n","Sentence: Businesses leverage NLP for customer service automation, market intelligence, and content generation.\n","Unigram Perplexity: 429.19017208176314\n","Bigram Perplexity: 88.50201568010462\n","Trigram Perplexity: 147.37894539596263\n","\n","Sentence: Researchers continue to explore new frontiers, such as explainable AI in NLP, multimodal NLP (combining text with images or audio), and ethical considerations in language models.\n","Unigram Perplexity: 484.2384550961289\n","Bigram Perplexity: 119.49224954490444\n","Trigram Perplexity: 180.48323098956803\n","\n","Sentence: Challenges in NLP include ambiguity (words with multiple meanings), sarcasm, irony, and understanding context-dependent information.\n","Unigram Perplexity: 543.1229913848848\n","Bigram Perplexity: 148.87822594146613\n","Trigram Perplexity: 173.81556644179994\n","\n","Sentence: The vast diversity of human languages and dialects also presents a significant hurdle.\n","Unigram Perplexity: 496.54026241655544\n","Bigram Perplexity: 94.880228922332\n","Trigram Perplexity: 167.2465250452465\n","\n","Sentence: Despite these challenges, the field of NLP continues to advance rapidly, promising even more sophisticated and human-like language understanding capabilities in the future.\n","Unigram Perplexity: 533.9545589742199\n","Bigram Perplexity: 133.80615624813902\n","Trigram Perplexity: 177.41527070173666\n","\n","Sentence: This text is a placeholder to ensure the minimum word count is met.\n","Unigram Perplexity: 676.7259879478688\n","Bigram Perplexity: 147.08591143523475\n","Trigram Perplexity: 165.87966941305953\n","\n","Sentence: It discusses the evolution and applications of Natural Language Processing (NLP).\n","Unigram Perplexity: 455.9153817270225\n","Bigram Perplexity: 140.77819534291638\n","Trigram Perplexity: 159.8312525539634\n","\n","Sentence: It highlights the progression from rule-based systems to statistical methods, and finally to deep learning architectures like Transformers.\n","Unigram Perplexity: 502.24257836991796\n","Bigram Perplexity: 112.63800821499673\n","Trigram Perplexity: 141.43441570266054\n","\n","Sentence: Key applications mentioned include virtual assistants, chatbots, machine translation, and sentiment analysis.\n","Unigram Perplexity: 329.7445879173067\n","Bigram Perplexity: 72.78317311526037\n","Trigram Perplexity: 116.64441243324625\n","\n","Sentence: The challenges of NLP, such as ambiguity and context understanding, are also touched upon.\n","Unigram Perplexity: 568.4747686011149\n","Bigram Perplexity: 139.89954567410322\n","Trigram Perplexity: 170.21187117400433\n","\n","Sentence: The field is dynamic, with continuous advancements pushing the boundaries of what machines can do with human language.\n","Unigram Perplexity: 645.7819431424313\n","Bigram Perplexity: 140.5895105747803\n","Trigram Perplexity: 173.36204787602702\n","\n"]}]},{"cell_type":"markdown","source":["Comparison and Analysis"],"metadata":{"id":"NeJNyawMViCP"}},{"cell_type":"markdown","source":["The trigram model generally produced the lowest perplexity values, indicating that it predicted test sentences more accurately by capturing richer contextual information. However, trigrams did not always perform best, especially when the training data was limited, due to data sparsity issues. In some cases, the bigram model achieved comparable or even lower perplexity than the trigram model because it required less contextual data. When unseen words or unseen N-gram combinations appeared in test sentences, the probability of those sentences decreased significantly. Without smoothing, unseen words would result in zero probability, making perplexity extremely high or undefined. Add-one (Laplace) smoothing helped resolve this issue by assigning small non-zero probabilities to unseen N-grams. Smoothing improved model robustness and allowed fair comparison across models. Overall, while higher-order models captured more context, bigram models provided a good balance between performance and reliability for the given dataset"],"metadata":{"id":"VIVe547eVt3Q"}},{"cell_type":"markdown","source":["Lab Report"],"metadata":{"id":"GeIvhjWzV3Xd"}},{"cell_type":"markdown","source":["Objective\n","\n","To implement Unigram, Bigram, and Trigram language models and evaluate them using sentence probability and perplexity.\n","\n","Dataset Description\n","\n","A general English text corpus of over 1500 words related to language and NLP was used. The dataset was split into 80% training and 20% testing data.\n","\n","Preprocessing Explanation\n","\n","Text was converted to lowercase, cleaned by removing punctuation and numbers, tokenized into words, and sentence boundary tokens were added.\n","\n","N-Gram Model Construction\n","\n","Unigram, Bigram, and Trigram models were built using word frequency counts and conditional probability calculations.\n","\n","Sentence Probability Results\n","\n","Unigram produced higher probabilities due to lack of context. Bigram and Trigram produced lower but more meaningful probabilities.\n","\n","Perplexity Comparison\n","\n","Trigram generally achieved the lowest perplexity, followed by Bigram and Unigram.\n","\n","Observations and Conclusion\n","\n","Higher-order models capture better context but need more data. Smoothing handled unseen words and improved model performance."],"metadata":{"id":"pyNMI-bFV-JK"}}]}