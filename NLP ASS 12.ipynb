{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1221KNfMwjzor3tBxrj1nB5w7u8sPrkJS","timestamp":1771496254603}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **STEP 2 — Import Libraries**"],"metadata":{"id":"kqusOgs_YI4C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9t-g_V-eTF6a"},"outputs":[],"source":["# Basic Libraries\n","import numpy as np\n","import pandas as pd\n","import re\n","import os\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# For padding\n","from collections import Counter"]},{"cell_type":"markdown","source":["# **STEP 3 — Load and Explore Dataset**"],"metadata":{"id":"5NqQVfu3YJdd"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75e9315a","executionInfo":{"status":"ok","timestamp":1771495696131,"user_tz":-330,"elapsed":11,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"b6df2e2d-9a83-42a8-e6fe-5604a08a9ebd"},"source":["# Load dataset\n","data = pd.read_csv('/content/SMSSpamCollection',\n","                   sep='\\t', header=None, names=['label', 'text'])\n","\n","print(data.head())\n","print(\"Total samples:\", len(data))\n","print(\"Class distribution:\\n\", data['label'].value_counts())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  label                                               text\n","0   ham  Go until jurong point, crazy.. Available only ...\n","1   ham                      Ok lar... Joking wif u oni...\n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n","3   ham  U dun say so early hor... U c already then say...\n","4   ham  Nah I don't think he goes to usf, he lives aro...\n","Total samples: 5572\n","Class distribution:\n"," label\n","ham     4825\n","spam     747\n","Name: count, dtype: int64\n"]}]},{"cell_type":"markdown","source":["# **STEP 4 — Text Preprocessing**"],"metadata":{"id":"hhzQ7KoLYVHj"}},{"cell_type":"code","source":["def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    return text\n","\n","data['text'] = data['text'].apply(clean_text)\n","\n","# Tokenization\n","data['tokens'] = data['text'].apply(lambda x: x.split())"],"metadata":{"id":"xCue1FzsUyk1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 50"],"metadata":{"id":"9WNxZjbmU64-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **STEP 5 — Vocabulary and Embedding Preparation**"],"metadata":{"id":"OV-iejQDYZ5n"}},{"cell_type":"code","source":["all_words = [word for tokens in data['tokens'] for word in tokens]\n","word_counts = Counter(all_words)\n","\n","vocab = {word: idx+1 for idx, (word, _) in enumerate(word_counts.items())}\n","vocab['<PAD>'] = 0\n","\n","vocab_size = len(vocab)"],"metadata":{"id":"39lypnhkU9BU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Download and Load GloVe**"],"metadata":{"id":"JSTuE6oPYg7L"}},{"cell_type":"code","source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4vJWVGqVLiB","executionInfo":{"status":"ok","timestamp":1771495695953,"user_tz":-330,"elapsed":375689,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"62a73db9-d8d8-42dd-bce6-25075a82423a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2026-02-19 10:02:23--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2026-02-19 10:02:24--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2026-02-19 10:02:24--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip.1’\n","\n","glove.6B.zip.1       88%[================>   ] 726.18M  5.19MB/s    eta 20s    ^C\n","Archive:  glove.6B.zip\n","replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ESWAR\n","error:  invalid response [ESWAR]\n","replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n","  inflating: glove.6B.50d.txt        \n","replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n","  inflating: glove.6B.100d.txt       Y\n","Y\n","\n","replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: glove.6B.200d.txt       Y\n","\n","replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: glove.6B.300d.txt       Y\n","\n"]}]},{"cell_type":"code","source":["def load_glove(glove_file, embedding_dim):\n","    embeddings = {}\n","    with open(glove_file, encoding='utf8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            embeddings[word] = vector\n","    return embeddings\n","\n","embedding_dim = 100\n","glove = load_glove('glove.6B.100d.txt', embedding_dim)"],"metadata":{"id":"tkRTO2pYV7Dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Embedding Matrix**"],"metadata":{"id":"8T-JT74dYlvG"}},{"cell_type":"code","source":["embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","for word, idx in vocab.items():\n","    vector = glove.get(word)\n","    if vector is not None:\n","        embedding_matrix[idx] = vector\n"],"metadata":{"id":"h-rh2NjdWTDJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **STEP 6 — Train-Test Split**"],"metadata":{"id":"naBHxbHqYvpA"}},{"cell_type":"code","source":["X = data['tokens']\n","y = data['label'].map({'ham':0, 'spam':1})\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"q30cHgriWX7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **STEP 7 — Build 1D CNN Model**"],"metadata":{"id":"8yKZ_obCYysu"}},{"cell_type":"code","source":["class SMSDataset(Dataset):\n","    def __init__(self, texts, labels, vocab, max_len):\n","        self.texts = texts\n","        self.labels = labels.values\n","        self.vocab = vocab\n","        self.max_len = max_len\n","\n","    def encode(self, tokens):\n","        encoded = [self.vocab.get(word, 0) for word in tokens]\n","        if len(encoded) < self.max_len:\n","            encoded += [0] * (self.max_len - len(encoded))\n","        else:\n","            encoded = encoded[:self.max_len]\n","        return torch.tensor(encoded)\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        return self.encode(self.texts.iloc[idx]), torch.tensor(self.labels[idx])\n"],"metadata":{"id":"akmpDTD4WdD_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **CNN Model**"],"metadata":{"id":"EevHJeCmY4_R"}},{"cell_type":"code","source":["class TextCNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, embedding_matrix):\n","        super(TextCNN, self).__init__()\n","\n","        self.embedding = nn.Embedding.from_pretrained(\n","            torch.tensor(embedding_matrix, dtype=torch.float32),\n","            freeze=False)\n","\n","        self.conv = nn.Conv1d(embedding_dim, 128, kernel_size=5)\n","        self.pool = nn.AdaptiveMaxPool1d(1)\n","        self.fc = nn.Linear(128, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.permute(0, 2, 1)\n","        x = torch.relu(self.conv(x))\n","        x = self.pool(x).squeeze(2)\n","        x = self.sigmoid(self.fc(x))\n","        return x\n"],"metadata":{"id":"Sf_Fx3Z6We6o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **STEP 8 — Model Training**"],"metadata":{"id":"yBkvHnpYY76P"}},{"cell_type":"code","source":["train_dataset = SMSDataset(X_train, y_train, vocab, MAX_LEN)\n","test_dataset = SMSDataset(X_test, y_test, vocab, MAX_LEN)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","model = TextCNN(vocab_size, embedding_dim, embedding_matrix)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(5):\n","    model.train()\n","    total_loss = 0\n","\n","    for inputs, labels in train_loader:\n","        labels = labels.float().unsqueeze(1)\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"540me9yCXD00","executionInfo":{"status":"ok","timestamp":1771495733422,"user_tz":-330,"elapsed":27121,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"76841b0f-aff0-4a4e-ba85-064bba72d66a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.20403586458414794\n","Epoch 2, Loss: 0.05254475425962093\n","Epoch 3, Loss: 0.02229563178261742\n","Epoch 4, Loss: 0.011001757635468883\n","Epoch 5, Loss: 0.005619726374321284\n"]}]},{"cell_type":"markdown","source":["# **STEP 9 — Model Evaluation**"],"metadata":{"id":"fLGWjj9GY-zt"}},{"cell_type":"code","source":["model.eval()\n","preds = []\n","true = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        outputs = model(inputs)\n","        predicted = (outputs > 0.5).int()\n","\n","        preds.extend(predicted.squeeze().tolist())\n","        true.extend(labels.tolist())\n","\n","acc = accuracy_score(true, preds)\n","prec = precision_score(true, preds)\n","rec = recall_score(true, preds)\n","f1 = f1_score(true, preds)\n","cm = confusion_matrix(true, preds)\n","\n","print(\"Accuracy:\", acc)\n","print(\"Precision:\", prec)\n","print(\"Recall:\", rec)\n","print(\"F1 Score:\", f1)\n","print(\"Confusion Matrix:\\n\", cm)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgVfGzFBXH9a","executionInfo":{"status":"ok","timestamp":1771495733749,"user_tz":-330,"elapsed":323,"user":{"displayName":"Eswar reddy Koduru","userId":"16456066281642142360"}},"outputId":"db5e1dc3-4766-4346-d723-85577ee1232d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9829596412556054\n","Precision: 0.9710144927536232\n","Recall: 0.8993288590604027\n","F1 Score: 0.9337979094076655\n","Confusion Matrix:\n"," [[962   4]\n"," [ 15 134]]\n"]}]},{"cell_type":"markdown","source":["# **STEP 10 — Result Analysis**"],"metadata":{"id":"yKe8un7ZYCuW"}},{"cell_type":"markdown","source":["The model achieved strong classification performance using pretrained GloVe embeddings. Pretrained embeddings improved semantic understanding because similar words share similar vector representations. Compared to random initialization, convergence was faster and loss decreased smoothly. The CNN was able to capture important local features such as spam keywords and patterns. The model handled short SMS texts effectively due to convolutional filters. Precision and recall values indicate good spam detection capability. However, rare words not present in GloVe were mapped to zero vectors, which may slightly reduce performance. The dataset imbalance can also influence evaluation metrics. Overall, pretrained embeddings significantly improved generalization compared to training embeddings from scratch."],"metadata":{"id":"KHwo_lEQYGzT"}}]}